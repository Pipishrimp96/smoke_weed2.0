{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout,LSTM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory growth enabled.\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# GPU 内存增长配置（解决 CUDA 内存不足问题）\n",
    "# ============================\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            # 设置每个GPU允许按需增长内存\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# ---------------------------\n",
    "# 1. 数据加载与预处理\n",
    "# ---------------------------\n",
    "# 假设你的CSV文件名为 \"qqq_data.csv\"\n",
    "data = pd.read_csv(\"D:\\python\\smoke_week_1.0_xgb\\smokeweed_2.0\\gru_model\\简单版方案1训练数据.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_1min</th>\n",
       "      <th>rsi_5min</th>\n",
       "      <th>rsi_30min</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>close_5min_change_pct</th>\n",
       "      <th>close_10min_change_pct</th>\n",
       "      <th>close_30min_change_pct</th>\n",
       "      <th>close_60min_change_pct</th>\n",
       "      <th>close_120min_change_pct</th>\n",
       "      <th>close_240min_change_pct</th>\n",
       "      <th>close_480min_change_pct</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-02 04:00:00</td>\n",
       "      <td>145.156</td>\n",
       "      <td>145.208</td>\n",
       "      <td>144.945</td>\n",
       "      <td>144.999</td>\n",
       "      <td>10183</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.030855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-02 04:01:00</td>\n",
       "      <td>145.060</td>\n",
       "      <td>145.111</td>\n",
       "      <td>144.888</td>\n",
       "      <td>145.086</td>\n",
       "      <td>4827</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.003198</td>\n",
       "      <td>0.029569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-02 04:02:00</td>\n",
       "      <td>145.060</td>\n",
       "      <td>145.150</td>\n",
       "      <td>144.984</td>\n",
       "      <td>145.125</td>\n",
       "      <td>4655</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>-0.000338</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>0.029292</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-02 04:03:00</td>\n",
       "      <td>145.156</td>\n",
       "      <td>145.227</td>\n",
       "      <td>145.013</td>\n",
       "      <td>145.202</td>\n",
       "      <td>7990</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.029015</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-02 04:04:00</td>\n",
       "      <td>145.166</td>\n",
       "      <td>145.285</td>\n",
       "      <td>145.100</td>\n",
       "      <td>145.260</td>\n",
       "      <td>3100</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000461</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49515</th>\n",
       "      <td>49515</td>\n",
       "      <td>2019-04-08 16:29:00</td>\n",
       "      <td>179.269</td>\n",
       "      <td>179.319</td>\n",
       "      <td>179.214</td>\n",
       "      <td>179.279</td>\n",
       "      <td>2600</td>\n",
       "      <td>55.122151</td>\n",
       "      <td>68.949241</td>\n",
       "      <td>70.369762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>-0.001567</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.002432</td>\n",
       "      <td>-0.003347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49516</th>\n",
       "      <td>49516</td>\n",
       "      <td>2019-04-08 16:30:00</td>\n",
       "      <td>179.250</td>\n",
       "      <td>179.299</td>\n",
       "      <td>179.176</td>\n",
       "      <td>179.240</td>\n",
       "      <td>5480</td>\n",
       "      <td>48.022460</td>\n",
       "      <td>64.354540</td>\n",
       "      <td>71.091005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>-0.001512</td>\n",
       "      <td>-0.001350</td>\n",
       "      <td>-0.002427</td>\n",
       "      <td>-0.003186</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49517</th>\n",
       "      <td>49517</td>\n",
       "      <td>2019-04-08 16:31:00</td>\n",
       "      <td>179.230</td>\n",
       "      <td>179.280</td>\n",
       "      <td>179.185</td>\n",
       "      <td>179.250</td>\n",
       "      <td>1000</td>\n",
       "      <td>49.807596</td>\n",
       "      <td>64.354540</td>\n",
       "      <td>71.091005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.001300</td>\n",
       "      <td>-0.002271</td>\n",
       "      <td>-0.003621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49518</th>\n",
       "      <td>49518</td>\n",
       "      <td>2019-04-08 16:32:00</td>\n",
       "      <td>179.240</td>\n",
       "      <td>179.299</td>\n",
       "      <td>179.195</td>\n",
       "      <td>179.269</td>\n",
       "      <td>10000</td>\n",
       "      <td>53.103215</td>\n",
       "      <td>64.354540</td>\n",
       "      <td>71.091005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001456</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>-0.001941</td>\n",
       "      <td>-0.003564</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49519</th>\n",
       "      <td>49519</td>\n",
       "      <td>2019-04-08 16:33:00</td>\n",
       "      <td>179.230</td>\n",
       "      <td>179.280</td>\n",
       "      <td>179.185</td>\n",
       "      <td>179.250</td>\n",
       "      <td>600</td>\n",
       "      <td>49.596242</td>\n",
       "      <td>64.354540</td>\n",
       "      <td>71.091005</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.001406</td>\n",
       "      <td>-0.001082</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>-0.003403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49520 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            timestamp     open     high      low    close  \\\n",
       "0               0  2019-01-02 04:00:00  145.156  145.208  144.945  144.999   \n",
       "1               1  2019-01-02 04:01:00  145.060  145.111  144.888  145.086   \n",
       "2               2  2019-01-02 04:02:00  145.060  145.150  144.984  145.125   \n",
       "3               3  2019-01-02 04:03:00  145.156  145.227  145.013  145.202   \n",
       "4               4  2019-01-02 04:04:00  145.166  145.285  145.100  145.260   \n",
       "...           ...                  ...      ...      ...      ...      ...   \n",
       "49515       49515  2019-04-08 16:29:00  179.269  179.319  179.214  179.279   \n",
       "49516       49516  2019-04-08 16:30:00  179.250  179.299  179.176  179.240   \n",
       "49517       49517  2019-04-08 16:31:00  179.230  179.280  179.185  179.250   \n",
       "49518       49518  2019-04-08 16:32:00  179.240  179.299  179.195  179.269   \n",
       "49519       49519  2019-04-08 16:33:00  179.230  179.280  179.185  179.250   \n",
       "\n",
       "       volume   rsi_1min   rsi_5min  rsi_30min  ...  day_of_week  month  \\\n",
       "0       10183  67.119465  71.548606  82.685061  ...            2      1   \n",
       "1        4827  67.119465  71.548606  82.685061  ...            2      1   \n",
       "2        4655  67.119465  71.548606  82.685061  ...            2      1   \n",
       "3        7990  67.119465  71.548606  82.685061  ...            2      1   \n",
       "4        3100  67.119465  71.548606  82.685061  ...            2      1   \n",
       "...       ...        ...        ...        ...  ...          ...    ...   \n",
       "49515    2600  55.122151  68.949241  70.369762  ...            0      4   \n",
       "49516    5480  48.022460  64.354540  71.091005  ...            0      4   \n",
       "49517    1000  49.807596  64.354540  71.091005  ...            0      4   \n",
       "49518   10000  53.103215  64.354540  71.091005  ...            0      4   \n",
       "49519     600  49.596242  64.354540  71.091005  ...            0      4   \n",
       "\n",
       "       close_5min_change_pct  close_10min_change_pct  close_30min_change_pct  \\\n",
       "0                   0.001931                0.001600                0.004131   \n",
       "1                   0.000000                0.000862                0.002598   \n",
       "2                   0.000200                0.000861               -0.000338   \n",
       "3                  -0.000269                0.000530               -0.000269   \n",
       "4                   0.000000                0.000461                0.000330   \n",
       "...                      ...                     ...                     ...   \n",
       "49515              -0.000162                0.000162                0.000268   \n",
       "49516               0.000000                0.000218                0.000379   \n",
       "49517               0.000162                0.000324                0.000106   \n",
       "49518               0.000056                0.000056                0.000000   \n",
       "49519               0.000268                0.000050               -0.000162   \n",
       "\n",
       "       close_60min_change_pct  close_120min_change_pct  \\\n",
       "0                    0.004862                 0.003000   \n",
       "1                    0.004528                 0.001930   \n",
       "2                    0.004589                 0.001991   \n",
       "3                    0.003526                 0.001660   \n",
       "4                    0.002058                 0.000799   \n",
       "...                       ...                      ...   \n",
       "49515               -0.001567                -0.001026   \n",
       "49516               -0.001512                -0.001350   \n",
       "49517               -0.001406                -0.001300   \n",
       "49518               -0.001456                -0.001188   \n",
       "49519               -0.001406                -0.001082   \n",
       "\n",
       "       close_240min_change_pct  close_480min_change_pct  target  \n",
       "0                     0.003400                 0.030855       0  \n",
       "1                     0.003198                 0.029569       0  \n",
       "2                     0.002329                 0.029292       0  \n",
       "3                     0.001928                 0.029015       0  \n",
       "4                     0.001397                 0.028535       0  \n",
       "...                        ...                      ...     ...  \n",
       "49515                -0.002432                -0.003347       0  \n",
       "49516                -0.002427                -0.003186       0  \n",
       "49517                -0.002271                -0.003621       0  \n",
       "49518                -0.001941                -0.003564       0  \n",
       "49519                -0.002215                -0.003403       0  \n",
       "\n",
       "[49520 rows x 41 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data[:50000]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算相对当前时间点5分钟、10分钟、30分钟之后的价格变化\n",
    "data['close_5min_change_pct'] = (data['close'].shift(-5) - data['close']) / data['close']\n",
    "data['close_10min_change_pct'] = (data['close'].shift(-10) - data['close']) / data['close']\n",
    "data['close_30min_change_pct'] = (data['close'].shift(-30) - data['close']) / data['close']\n",
    "data['close_60min_change_pct'] = (data['close'].shift(-60) - data['close']) / data['close']\n",
    "data['close_120min_change_pct'] = (data['close'].shift(-120) - data['close']) / data['close']\n",
    "data['close_240min_change_pct'] = (data['close'].shift(-240) - data['close']) / data['close']\n",
    "data['close_480min_change_pct'] = (data['close'].shift(-480) - data['close']) / data['close']\n",
    "# 去除包含目标标签 NaN 的行\n",
    "data.dropna(subset=['close_5min_change_pct', 'close_10min_change_pct', 'close_30min_change_pct','close_60min_change_pct','close_120min_change_pct','close_240min_change_pct','close_480min_change_pct'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义分类条件\n",
    "conditions = [\n",
    "    (data['close_240min_change_pct'] > 0.008),                  # 数值 > 0.008\n",
    "    (data['close_240min_change_pct'] < -0.008),                 # 数值 < -0.008\n",
    "    (data['close_240min_change_pct'].between(-0.008, 0.008))    # -0.008 < 数值 < 0.008\n",
    "]\n",
    "\n",
    "# 定义对应的分类标签\n",
    "choices = [1, 2, 0]\n",
    "\n",
    "# 创建新列存储分类结果\n",
    "data['target'] = np.select(conditions, choices, default=0)  # default=0 是为了处理可能的边缘情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    41966\n",
       "1     3935\n",
       "2     2659\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计每个类别的数量\n",
    "category_counts = data['target'].value_counts()\n",
    "category_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all = data['target'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.19054778, 0.5135692, 0.0, 2.0)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_all), np.std(y_all), np.min(y_all), np.max(y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_all = to_categorical(data['target'], num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48560,)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all[8889]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi_1min</th>\n",
       "      <th>rsi_5min</th>\n",
       "      <th>rsi_30min</th>\n",
       "      <th>rsi_60min</th>\n",
       "      <th>rsi_4h</th>\n",
       "      <th>...</th>\n",
       "      <th>close_return_10</th>\n",
       "      <th>close_return_30</th>\n",
       "      <th>close_return_60</th>\n",
       "      <th>close_return_240</th>\n",
       "      <th>close_return_960</th>\n",
       "      <th>close_return_4800</th>\n",
       "      <th>close_return_19200</th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145.156</td>\n",
       "      <td>145.208</td>\n",
       "      <td>144.945</td>\n",
       "      <td>144.999</td>\n",
       "      <td>10183</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>46.513628</td>\n",
       "      <td>69.498846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>145.060</td>\n",
       "      <td>145.111</td>\n",
       "      <td>144.888</td>\n",
       "      <td>145.086</td>\n",
       "      <td>4827</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>46.513628</td>\n",
       "      <td>69.498846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>145.060</td>\n",
       "      <td>145.150</td>\n",
       "      <td>144.984</td>\n",
       "      <td>145.125</td>\n",
       "      <td>4655</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>46.513628</td>\n",
       "      <td>69.498846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>145.156</td>\n",
       "      <td>145.227</td>\n",
       "      <td>145.013</td>\n",
       "      <td>145.202</td>\n",
       "      <td>7990</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>46.513628</td>\n",
       "      <td>69.498846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145.166</td>\n",
       "      <td>145.285</td>\n",
       "      <td>145.100</td>\n",
       "      <td>145.260</td>\n",
       "      <td>3100</td>\n",
       "      <td>67.119465</td>\n",
       "      <td>71.548606</td>\n",
       "      <td>82.685061</td>\n",
       "      <td>46.513628</td>\n",
       "      <td>69.498846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.004662</td>\n",
       "      <td>0.073594</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48555</th>\n",
       "      <td>178.640</td>\n",
       "      <td>178.689</td>\n",
       "      <td>178.508</td>\n",
       "      <td>178.572</td>\n",
       "      <td>19844</td>\n",
       "      <td>40.888351</td>\n",
       "      <td>48.394595</td>\n",
       "      <td>71.695017</td>\n",
       "      <td>68.408398</td>\n",
       "      <td>71.056057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>-0.000599</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.040848</td>\n",
       "      <td>0.064234</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48556</th>\n",
       "      <td>178.553</td>\n",
       "      <td>178.612</td>\n",
       "      <td>178.421</td>\n",
       "      <td>178.572</td>\n",
       "      <td>58531</td>\n",
       "      <td>40.888351</td>\n",
       "      <td>48.394595</td>\n",
       "      <td>71.695017</td>\n",
       "      <td>68.408398</td>\n",
       "      <td>71.056057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>-0.000437</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.040617</td>\n",
       "      <td>0.064297</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48557</th>\n",
       "      <td>178.562</td>\n",
       "      <td>178.631</td>\n",
       "      <td>178.469</td>\n",
       "      <td>178.553</td>\n",
       "      <td>77552</td>\n",
       "      <td>38.662300</td>\n",
       "      <td>48.394595</td>\n",
       "      <td>71.695017</td>\n",
       "      <td>68.408398</td>\n",
       "      <td>71.056057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.040143</td>\n",
       "      <td>0.064368</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48558</th>\n",
       "      <td>178.533</td>\n",
       "      <td>178.593</td>\n",
       "      <td>178.411</td>\n",
       "      <td>178.553</td>\n",
       "      <td>53825</td>\n",
       "      <td>38.662300</td>\n",
       "      <td>53.189580</td>\n",
       "      <td>71.695017</td>\n",
       "      <td>68.408398</td>\n",
       "      <td>71.056057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000565</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>0.038630</td>\n",
       "      <td>0.064127</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48559</th>\n",
       "      <td>178.543</td>\n",
       "      <td>178.602</td>\n",
       "      <td>178.481</td>\n",
       "      <td>178.563</td>\n",
       "      <td>24871</td>\n",
       "      <td>40.635091</td>\n",
       "      <td>53.189580</td>\n",
       "      <td>71.695017</td>\n",
       "      <td>68.408398</td>\n",
       "      <td>71.056057</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>-0.000593</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>0.003050</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.037343</td>\n",
       "      <td>0.063451</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48560 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          open     high      low    close  volume   rsi_1min   rsi_5min  \\\n",
       "0      145.156  145.208  144.945  144.999   10183  67.119465  71.548606   \n",
       "1      145.060  145.111  144.888  145.086    4827  67.119465  71.548606   \n",
       "2      145.060  145.150  144.984  145.125    4655  67.119465  71.548606   \n",
       "3      145.156  145.227  145.013  145.202    7990  67.119465  71.548606   \n",
       "4      145.166  145.285  145.100  145.260    3100  67.119465  71.548606   \n",
       "...        ...      ...      ...      ...     ...        ...        ...   \n",
       "48555  178.640  178.689  178.508  178.572   19844  40.888351  48.394595   \n",
       "48556  178.553  178.612  178.421  178.572   58531  40.888351  48.394595   \n",
       "48557  178.562  178.631  178.469  178.553   77552  38.662300  48.394595   \n",
       "48558  178.533  178.593  178.411  178.553   53825  38.662300  53.189580   \n",
       "48559  178.543  178.602  178.481  178.563   24871  40.635091  53.189580   \n",
       "\n",
       "       rsi_30min  rsi_60min     rsi_4h  ...  close_return_10  close_return_30  \\\n",
       "0      82.685061  46.513628  69.498846  ...         0.001600         0.004131   \n",
       "1      82.685061  46.513628  69.498846  ...         0.001600         0.004131   \n",
       "2      82.685061  46.513628  69.498846  ...         0.001600         0.004131   \n",
       "3      82.685061  46.513628  69.498846  ...         0.001600         0.004131   \n",
       "4      82.685061  46.513628  69.498846  ...         0.001600         0.004131   \n",
       "...          ...        ...        ...  ...              ...              ...   \n",
       "48555  71.695017  68.408398  71.056057  ...        -0.000162        -0.000599   \n",
       "48556  71.695017  68.408398  71.056057  ...        -0.000437        -0.000437   \n",
       "48557  71.695017  68.408398  71.056057  ...        -0.000593        -0.000151   \n",
       "48558  71.695017  68.408398  71.056057  ...        -0.000565        -0.000162   \n",
       "48559  71.695017  68.408398  71.056057  ...        -0.000487        -0.000593   \n",
       "\n",
       "       close_return_60  close_return_240  close_return_960  close_return_4800  \\\n",
       "0             0.004862          0.003400          0.004662           0.073594   \n",
       "1             0.004862          0.003400          0.004662           0.073594   \n",
       "2             0.004862          0.003400          0.004662           0.073594   \n",
       "3             0.004862          0.003400          0.004662           0.073594   \n",
       "4             0.004862          0.003400          0.004662           0.073594   \n",
       "...                ...               ...               ...                ...   \n",
       "48555         0.000218          0.003044          0.003535           0.040848   \n",
       "48556         0.000381          0.003044          0.003642           0.040617   \n",
       "48557         0.000325          0.002994          0.003535           0.040143   \n",
       "48558         0.000112          0.002994          0.003535           0.038630   \n",
       "48559        -0.000341          0.003050          0.003377           0.037343   \n",
       "\n",
       "       close_return_19200  hour  day_of_week  month  \n",
       "0                0.137408     4            2      1  \n",
       "1                0.137408     4            2      1  \n",
       "2                0.137408     4            2      1  \n",
       "3                0.137408     4            2      1  \n",
       "4                0.137408     4            2      1  \n",
       "...                   ...   ...          ...    ...  \n",
       "48555            0.064234    11            4      4  \n",
       "48556            0.064297    11            4      4  \n",
       "48557            0.064368    11            4      4  \n",
       "48558            0.064127    11            4      4  \n",
       "48559            0.063451    11            4      4  \n",
       "\n",
       "[48560 rows x 31 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = data.iloc[:,2:33\n",
    "]\n",
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征矩阵和目标向量\n",
    "X_all = feature_cols.values.astype(np.float32)\n",
    "# 标准化特征（目标这里不进行归一化）\n",
    "scaler_X = StandardScaler()\n",
    "X_all = scaler_X.fit_transform(X_all)\n",
    "# ---------------------------\n",
    "# 2. 构造时间序列数据\n",
    "# ---------------------------\n",
    "seq_length = 480  # 使用过去480个时间步预测第11个时刻\n",
    "# 总样本数 = len(X_all) - seq_length\n",
    "total_samples = len(X_all) - seq_length\n",
    "def sequence_generator(indices, X, y, seq_length):\n",
    "    \"\"\"\n",
    "    按给定 indices 生成滑动窗口数据，每个样本由连续 seq_length 个时间步构成，\n",
    "    目标为窗口后一个时刻的 y 值。\n",
    "    \"\"\"\n",
    "    for i in indices:\n",
    "        # 生成一个样本：X[i:i+seq_length] 和 y[i+seq_length]\n",
    "        idx = int(i)  # 显式转换为 Python 的 int 类型\n",
    "        yield X[idx: idx + seq_length], np.array([y[idx + seq_length]])\n",
    "\n",
    "# 定义输出签名（shape 和数据类型），便于 tf.data.Dataset.from_generator 使用\n",
    "output_signature = (\n",
    "    tf.TensorSpec(shape=(seq_length, X_all.shape[1]), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(1,), dtype=tf.float32)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 3. 使用 TimeSeriesSplit 进行交叉验证\n",
    "# ---------------------------\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "fold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "\n",
    "class PerClassPrecision(Metric):\n",
    "    def __init__(self, class_idx, name, **kwargs):\n",
    "        super(PerClassPrecision, self).__init__(name=name, **kwargs)\n",
    "        self.class_idx = class_idx\n",
    "        # 累积整个 epoch 的正确预测数和预测总数\n",
    "        self.total_correct = self.add_weight(name='total_correct', initializer='zeros')\n",
    "        self.total_predicted = self.add_weight(name='total_predicted', initializer='zeros')\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # 处理输入\n",
    "        y_true = tf.cast(tf.squeeze(y_true), tf.int64)\n",
    "        y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int64)\n",
    "        \n",
    "        # 标记预测为该类的样本\n",
    "        mask = tf.cast(tf.equal(y_pred_classes, self.class_idx), tf.float32)\n",
    "        num_predicted = tf.reduce_sum(mask)\n",
    "        \n",
    "        # 累积预测总数\n",
    "        self.total_predicted.assign_add(num_predicted)\n",
    "        \n",
    "        # 计算并累积正确预测数\n",
    "        if num_predicted > 0:\n",
    "            correct = tf.cast(tf.equal(y_true, self.class_idx), tf.float32)\n",
    "            correct_predicted = tf.reduce_sum(correct * mask)\n",
    "            self.total_correct.assign_add(correct_predicted)\n",
    "    def result(self):\n",
    "        # 计算整个 epoch 的精确率\n",
    "        return tf.cond(tf.equal(self.total_predicted, 0.0),\n",
    "                       lambda: 0.0,  # 若整个 epoch 未预测该类，返回 0\n",
    "                       lambda: self.total_correct / self.total_predicted)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        # 重置状态（每个 epoch 开始时调用）\n",
    "        self.total_correct.assign(0.0)\n",
    "        self.total_predicted.assign(0.0)\n",
    "\n",
    "# 定义指标\n",
    "class0_acc_metric = PerClassPrecision(0, name='class0_accuracy')\n",
    "class1_acc_metric = PerClassPrecision(1, name='class1_accuracy')\n",
    "class2_acc_metric = PerClassPrecision(2, name='class2_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    " ###有bug   def per_class_accuracy(class_idx):\n",
    "        \"\"\"返回一个计算指定类别准确率的指标函数。\"\"\"\n",
    "        def accuracy_fn(y_true, y_pred):\n",
    "            # 将 y_true squeeze 成一维向量，确保数据类型为 int64\n",
    "            y_true = tf.cast(tf.squeeze(y_true), tf.int64)\n",
    "            # 获取预测类别（使用 argmax 得到概率最高的类别）\n",
    "            y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int64)\n",
    "            # 创建 mask，标记模型预测为 class_idx 的样本\n",
    "            mask = tf.equal(y_pred_classes, class_idx)\n",
    "            mask = tf.cast(mask, tf.float32)\n",
    "            # 计算预测为 class_idx 的总次数\n",
    "            num_predicted = tf.reduce_sum(mask)\n",
    "            def no_predictions():\n",
    "                return 0.0  # 若无预测为该类的样本，返回 -1.0\n",
    "            \n",
    "            def compute_accuracy():\n",
    "                # 检查预测为 class_idx 的样本中，真实标签是否正确\n",
    "                correct = tf.cast(tf.equal(y_true, class_idx), tf.float32)\n",
    "                correct_predicted = tf.reduce_sum(correct * mask)\n",
    "                return correct_predicted / num_predicted\n",
    "            \n",
    "            # 若无预测为该类的样本，避免除零\n",
    "            return tf.cond(tf.equal(num_predicted, 0.0), no_predictions, compute_accuracy)\n",
    "        \n",
    "        return accuracy_fn\n",
    "\n",
    "    # 利用 Keras 的 MeanMetricWrapper 包装自定义指标\n",
    "    class0_acc_metric = tf.keras.metrics.MeanMetricWrapper(per_class_accuracy(0), name='class0_accuracy')\n",
    "    class1_acc_metric = tf.keras.metrics.MeanMetricWrapper(per_class_accuracy(1), name='class1_accuracy')\n",
    "    class2_acc_metric = tf.keras.metrics.MeanMetricWrapper(per_class_accuracy(2), name='class2_accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型的函数（参考 model.py 超参数设置）\n",
    "input_dim = X_all.shape[1]\n",
    "def build_model(input_shape):\n",
    "    model = Sequential()\n",
    "    # GRU层：64个隐藏单元，dropout=0.3，返回最后一个时间步输出\n",
    "    model.add(LSTM(256, return_sequences=False, input_shape=input_shape,kernel_regularizer=regularizers.l2(0.004)))\n",
    "    #model.add(LSTM(256, return_sequences=False))\n",
    "    #model.add(GRU(units=32,return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.004)))\n",
    "    model.add(Dropout(0.2))\n",
    "    # 输出层3个神经元，对应三个类别，使用 softmax 激活\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    # 使用 categorical crossentropy 损失\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy', class1_acc_metric, class2_acc_metric, class0_acc_metric])\n",
    "    return model\n",
    "\n",
    "# 定义类别权重，按类别样本比例（或业务重要性）设置\n",
    "class_weight = {0: 1.0, 1: 100, 2: 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fold 1 ---\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_32 (LSTM)              (None, 256)               294912    \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,475\n",
      "Trainable params: 361,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric PerClassPrecision implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
      "  m.reset_state()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501/501 - 24s - loss: 54.7675 - accuracy: 0.7731 - class1_accuracy: 0.5076 - class2_accuracy: 0.4452 - class0_accuracy: 0.8630 - val_loss: 1.9049 - val_accuracy: 0.7745 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7757 - lr: 0.0010 - 24s/epoch - 47ms/step\n",
      "Epoch 2/200\n",
      "501/501 - 24s - loss: 55.3183 - accuracy: 0.7777 - class1_accuracy: 0.5415 - class2_accuracy: 0.4490 - class0_accuracy: 0.8505 - val_loss: 1.7288 - val_accuracy: 0.3726 - val_class1_accuracy: 0.1016 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7958 - lr: 0.0010 - 24s/epoch - 47ms/step\n",
      "Epoch 3/200\n",
      "501/501 - 25s - loss: 64.3483 - accuracy: 0.7210 - class1_accuracy: 0.3514 - class2_accuracy: 0.3222 - class0_accuracy: 0.8334 - val_loss: 1.7207 - val_accuracy: 0.5133 - val_class1_accuracy: 0.0428 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7973 - lr: 0.0010 - 25s/epoch - 50ms/step\n",
      "Epoch 4/200\n",
      "501/501 - 27s - loss: 55.7303 - accuracy: 0.7210 - class1_accuracy: 0.4483 - class2_accuracy: 0.2408 - class0_accuracy: 0.8461 - val_loss: 6.2392 - val_accuracy: 0.4148 - val_class1_accuracy: 0.0775 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7899 - lr: 0.0010 - 27s/epoch - 54ms/step\n",
      "Epoch 5/200\n",
      "501/501 - 27s - loss: 54.4005 - accuracy: 0.7329 - class1_accuracy: 0.4356 - class2_accuracy: 0.3053 - class0_accuracy: 0.8328 - val_loss: 2.0128 - val_accuracy: 0.3004 - val_class1_accuracy: 0.1059 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7757 - lr: 0.0010 - 27s/epoch - 54ms/step\n",
      "Epoch 6/200\n",
      "501/501 - 28s - loss: 40.9330 - accuracy: 0.7454 - class1_accuracy: 0.4705 - class2_accuracy: 0.4289 - class0_accuracy: 0.8713 - val_loss: 2.4268 - val_accuracy: 0.5325 - val_class1_accuracy: 0.0404 - val_class2_accuracy: 1.0000 - val_class0_accuracy: 0.7923 - lr: 0.0010 - 28s/epoch - 56ms/step\n",
      "Epoch 7/200\n",
      "501/501 - 29s - loss: 68.6919 - accuracy: 0.7087 - class1_accuracy: 0.3774 - class2_accuracy: 0.1879 - class0_accuracy: 0.8530 - val_loss: 1.6780 - val_accuracy: 0.7598 - val_class1_accuracy: 0.0256 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7743 - lr: 5.0000e-04 - 29s/epoch - 58ms/step\n",
      "Epoch 8/200\n",
      "501/501 - 29s - loss: 66.3350 - accuracy: 0.7515 - class1_accuracy: 0.5043 - class2_accuracy: 0.3769 - class0_accuracy: 0.8007 - val_loss: 1.0236 - val_accuracy: 0.7759 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7759 - lr: 5.0000e-04 - 29s/epoch - 59ms/step\n",
      "Epoch 9/200\n",
      "501/501 - 30s - loss: 49.2898 - accuracy: 0.7817 - class1_accuracy: 0.5289 - class2_accuracy: 0.3742 - class0_accuracy: 0.8378 - val_loss: 2.6465 - val_accuracy: 0.3936 - val_class1_accuracy: 0.0784 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7828 - lr: 5.0000e-04 - 30s/epoch - 60ms/step\n",
      "Epoch 10/200\n",
      "501/501 - 30s - loss: 58.8564 - accuracy: 0.6220 - class1_accuracy: 0.2563 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8645 - val_loss: 1.0668 - val_accuracy: 0.7759 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7759 - lr: 5.0000e-04 - 30s/epoch - 59ms/step\n",
      "Epoch 11/200\n",
      "501/501 - 30s - loss: 54.8709 - accuracy: 0.7551 - class1_accuracy: 0.3740 - class2_accuracy: 0.4519 - class0_accuracy: 0.8372 - val_loss: 1.2284 - val_accuracy: 0.4745 - val_class1_accuracy: 0.0382 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7843 - lr: 5.0000e-04 - 30s/epoch - 60ms/step\n",
      "Epoch 12/200\n",
      "501/501 - 29s - loss: 59.7089 - accuracy: 0.7790 - class1_accuracy: 0.6351 - class2_accuracy: 0.2355 - class0_accuracy: 0.8115 - val_loss: 1.1986 - val_accuracy: 0.4677 - val_class1_accuracy: 0.0462 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7211 - lr: 2.5000e-04 - 29s/epoch - 57ms/step\n",
      "Epoch 13/200\n",
      "501/501 - 29s - loss: 53.2010 - accuracy: 0.8017 - class1_accuracy: 0.5964 - class2_accuracy: 0.4312 - class0_accuracy: 0.8413 - val_loss: 1.1947 - val_accuracy: 0.5013 - val_class1_accuracy: 0.1053 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7549 - lr: 2.5000e-04 - 29s/epoch - 58ms/step\n",
      "Epoch 14/200\n",
      "501/501 - 29s - loss: 53.2059 - accuracy: 0.7840 - class1_accuracy: 0.4184 - class2_accuracy: 0.3639 - class0_accuracy: 0.8406 - val_loss: 1.1852 - val_accuracy: 0.4993 - val_class1_accuracy: 0.0790 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7456 - lr: 2.5000e-04 - 29s/epoch - 57ms/step\n",
      "Epoch 15/200\n",
      "501/501 - 29s - loss: 51.0064 - accuracy: 0.8099 - class1_accuracy: 0.6220 - class2_accuracy: 0.5122 - class0_accuracy: 0.8410 - val_loss: 1.3240 - val_accuracy: 0.4549 - val_class1_accuracy: 0.0790 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7343 - lr: 1.2500e-04 - 29s/epoch - 58ms/step\n",
      "Epoch 16/200\n",
      "501/501 - 30s - loss: 50.3301 - accuracy: 0.7968 - class1_accuracy: 0.5313 - class2_accuracy: 0.4117 - class0_accuracy: 0.8427 - val_loss: 1.3826 - val_accuracy: 0.4189 - val_class1_accuracy: 0.0872 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7181 - lr: 1.2500e-04 - 30s/epoch - 59ms/step\n",
      "Epoch 17/200\n",
      "501/501 - 30s - loss: 41.2373 - accuracy: 0.8250 - class1_accuracy: 0.6176 - class2_accuracy: 0.5500 - class0_accuracy: 0.8648 - val_loss: 1.5379 - val_accuracy: 0.4263 - val_class1_accuracy: 0.1345 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7391 - lr: 1.2500e-04 - 30s/epoch - 60ms/step\n",
      "Epoch 18/200\n",
      "501/501 - 30s - loss: 35.4957 - accuracy: 0.8319 - class1_accuracy: 0.6847 - class2_accuracy: 0.5967 - class0_accuracy: 0.8686 - val_loss: 1.7363 - val_accuracy: 0.3638 - val_class1_accuracy: 0.1365 - val_class2_accuracy: 0.0140 - val_class0_accuracy: 0.7224 - lr: 6.2500e-05 - 30s/epoch - 61ms/step\n",
      "Epoch 19/200\n",
      "501/501 - 31s - loss: 34.7507 - accuracy: 0.8453 - class1_accuracy: 0.7469 - class2_accuracy: 0.6475 - class0_accuracy: 0.8714 - val_loss: 1.9324 - val_accuracy: 0.3744 - val_class1_accuracy: 0.1146 - val_class2_accuracy: 0.3429 - val_class0_accuracy: 0.7675 - lr: 6.2500e-05 - 31s/epoch - 61ms/step\n",
      "Epoch 20/200\n",
      "501/501 - 30s - loss: 34.2988 - accuracy: 0.7648 - class1_accuracy: 0.3832 - class2_accuracy: 0.6561 - class0_accuracy: 0.8574 - val_loss: 1.7751 - val_accuracy: 0.3476 - val_class1_accuracy: 0.0963 - val_class2_accuracy: 0.3546 - val_class0_accuracy: 0.8125 - lr: 6.2500e-05 - 30s/epoch - 59ms/step\n",
      "Epoch 21/200\n",
      "501/501 - 30s - loss: 34.1080 - accuracy: 0.8469 - class1_accuracy: 0.6439 - class2_accuracy: 0.7893 - class0_accuracy: 0.8743 - val_loss: 1.8240 - val_accuracy: 0.3674 - val_class1_accuracy: 0.1081 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.7408 - lr: 3.1250e-05 - 30s/epoch - 59ms/step\n",
      "Epoch 22/200\n",
      "501/501 - 30s - loss: 30.6709 - accuracy: 0.8684 - class1_accuracy: 0.7309 - class2_accuracy: 0.8097 - class0_accuracy: 0.8896 - val_loss: 1.9732 - val_accuracy: 0.3404 - val_class1_accuracy: 0.1032 - val_class2_accuracy: 0.5870 - val_class0_accuracy: 0.7233 - lr: 3.1250e-05 - 30s/epoch - 60ms/step\n",
      "Epoch 23/200\n",
      "501/501 - 31s - loss: 30.0053 - accuracy: 0.8717 - class1_accuracy: 0.7490 - class2_accuracy: 0.7877 - class0_accuracy: 0.8946 - val_loss: 1.9681 - val_accuracy: 0.3829 - val_class1_accuracy: 0.1090 - val_class2_accuracy: 0.4763 - val_class0_accuracy: 0.7569 - lr: 3.1250e-05 - 31s/epoch - 61ms/step\n",
      "Epoch 24/200\n",
      "501/501 - 31s - loss: 28.9976 - accuracy: 0.8823 - class1_accuracy: 0.7731 - class2_accuracy: 0.8591 - class0_accuracy: 0.8970 - val_loss: 1.9419 - val_accuracy: 0.3987 - val_class1_accuracy: 0.1141 - val_class2_accuracy: 0.5144 - val_class0_accuracy: 0.7598 - lr: 1.5625e-05 - 31s/epoch - 61ms/step\n",
      "Epoch 25/200\n",
      "501/501 - 31s - loss: 28.1734 - accuracy: 0.8802 - class1_accuracy: 0.7804 - class2_accuracy: 0.8344 - class0_accuracy: 0.8957 - val_loss: 1.9951 - val_accuracy: 0.3969 - val_class1_accuracy: 0.1136 - val_class2_accuracy: 0.5092 - val_class0_accuracy: 0.7582 - lr: 1.5625e-05 - 31s/epoch - 61ms/step\n",
      "Epoch 26/200\n",
      "501/501 - 30s - loss: 27.4107 - accuracy: 0.8802 - class1_accuracy: 0.7827 - class2_accuracy: 0.7973 - class0_accuracy: 0.9003 - val_loss: 2.1382 - val_accuracy: 0.3979 - val_class1_accuracy: 0.1181 - val_class2_accuracy: 0.3633 - val_class0_accuracy: 0.7762 - lr: 1.5625e-05 - 30s/epoch - 61ms/step\n",
      "Epoch 27/200\n",
      "501/501 - 30s - loss: 27.3921 - accuracy: 0.8851 - class1_accuracy: 0.7963 - class2_accuracy: 0.8267 - class0_accuracy: 0.9012 - val_loss: 2.3599 - val_accuracy: 0.3514 - val_class1_accuracy: 0.1058 - val_class2_accuracy: 0.3359 - val_class0_accuracy: 0.8010 - lr: 7.8125e-06 - 30s/epoch - 60ms/step\n",
      "Epoch 28/200\n",
      "501/501 - 30s - loss: 26.5783 - accuracy: 0.8850 - class1_accuracy: 0.8010 - class2_accuracy: 0.8066 - class0_accuracy: 0.9027 - val_loss: 2.4352 - val_accuracy: 0.3471 - val_class1_accuracy: 0.1103 - val_class2_accuracy: 0.3029 - val_class0_accuracy: 0.7975 - lr: 7.8125e-06 - 30s/epoch - 60ms/step\n",
      "Epoch 29/200\n",
      "501/501 - 30s - loss: 32.5715 - accuracy: 0.8710 - class1_accuracy: 0.7118 - class2_accuracy: 0.7421 - class0_accuracy: 0.9039 - val_loss: 2.4742 - val_accuracy: 0.3478 - val_class1_accuracy: 0.1131 - val_class2_accuracy: 0.3120 - val_class0_accuracy: 0.7943 - lr: 7.8125e-06 - 30s/epoch - 59ms/step\n",
      "Epoch 30/200\n",
      "501/501 - 30s - loss: 26.1767 - accuracy: 0.8785 - class1_accuracy: 0.7948 - class2_accuracy: 0.6882 - class0_accuracy: 0.9112 - val_loss: 2.5019 - val_accuracy: 0.3514 - val_class1_accuracy: 0.1120 - val_class2_accuracy: 0.3259 - val_class0_accuracy: 0.8148 - lr: 3.9063e-06 - 30s/epoch - 59ms/step\n",
      "Epoch 31/200\n",
      "501/501 - 30s - loss: 25.6553 - accuracy: 0.8816 - class1_accuracy: 0.8140 - class2_accuracy: 0.7323 - class0_accuracy: 0.9066 - val_loss: 2.4906 - val_accuracy: 0.3513 - val_class1_accuracy: 0.1119 - val_class2_accuracy: 0.3562 - val_class0_accuracy: 0.8106 - lr: 3.9063e-06 - 30s/epoch - 60ms/step\n",
      "Epoch 32/200\n",
      "501/501 - 30s - loss: 25.6549 - accuracy: 0.8802 - class1_accuracy: 0.8005 - class2_accuracy: 0.7326 - class0_accuracy: 0.9069 - val_loss: 2.5164 - val_accuracy: 0.3517 - val_class1_accuracy: 0.1128 - val_class2_accuracy: 0.3488 - val_class0_accuracy: 0.8106 - lr: 3.9063e-06 - 30s/epoch - 60ms/step\n",
      "Epoch 33/200\n",
      "501/501 - 29s - loss: 25.1932 - accuracy: 0.8786 - class1_accuracy: 0.7861 - class2_accuracy: 0.7182 - class0_accuracy: 0.9091 - val_loss: 2.5546 - val_accuracy: 0.3517 - val_class1_accuracy: 0.1131 - val_class2_accuracy: 0.3483 - val_class0_accuracy: 0.8108 - lr: 1.9531e-06 - 29s/epoch - 58ms/step\n",
      "Epoch 34/200\n",
      "501/501 - 34s - loss: 25.0828 - accuracy: 0.8795 - class1_accuracy: 0.7904 - class2_accuracy: 0.7479 - class0_accuracy: 0.9050 - val_loss: 2.5935 - val_accuracy: 0.3477 - val_class1_accuracy: 0.1132 - val_class2_accuracy: 0.3372 - val_class0_accuracy: 0.8084 - lr: 1.9531e-06 - 34s/epoch - 68ms/step\n",
      "Epoch 35/200\n",
      "501/501 - 36s - loss: 24.8211 - accuracy: 0.8845 - class1_accuracy: 0.8174 - class2_accuracy: 0.7457 - class0_accuracy: 0.9086 - val_loss: 2.6417 - val_accuracy: 0.3469 - val_class1_accuracy: 0.1135 - val_class2_accuracy: 0.3352 - val_class0_accuracy: 0.8076 - lr: 1.9531e-06 - 36s/epoch - 73ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVuJJREFUeJzt3Qd4VFX6P/Bveu+VQEINXZBOkKJUWQsK9oY/C6siK+DuKvu3u4plLasrYltYXQHFFayg0hFCF+mhk0BIIZDek/k/75m5wyQkkEmmz/fzPJe5c2cyOZlcct855z3v8dDpdDoQERER2Yinrb4RERERkWDwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNecPB1NbWIjMzEyEhIfDw8LB3c4iIiKgJpGZpUVEREhIS4Onp6VzBhwQeiYmJ9m4GERERNUNGRgbatGnjXMGH9HhojQ8NDbV3c4iIiKgJCgsLVeeBdh13quBDG2qRwIPBBxERkXNpSsoEE06JiIjIphh8EBERkU0x+CAiIiKbMivno127djhx4sQFxx955BG89957KC8vx+OPP45FixahoqIC48aNw5w5cxAXF2fJNhMRkRNOw6yurkZNTY29m0It4OPjAy8vL9g0+Ni6dWudE2fPnj0YM2YMbr75ZnV/xowZ+OGHH7B48WKEhYXh0UcfxcSJE7Fhw4YWN5SIiJxTZWUlTp8+jdLSUns3hSyQTCrTaIODg1v2OjoJR5tp+vTp+P7773Ho0CE1xSYmJgYLFizATTfdpB4/cOAAunXrhtTUVAwePLhJrymvI4FLQUEBZ7sQETk5KRwp1wj5tCzXCF9fXxaQdFISLuTm5qogMjk5+YIeEHOu394tiWT/+9//YubMmepE2r59O6qqqjB69Gjjc7p27YqkpKSLBh8yPCObaeOJiMg1yLVCAhCp/xAYGGjv5lALSQB5/Phxdb1vyfBLsxNOly5divz8fNx7773qflZWlopow8PD6zxP8j3kscbMnj1bRUraxuqmRESu51Lltsk5WKrXqtlnwyeffILx48erGu4tMWvWLNVFo21S2ZSIiIhcV7OGXWTGy4oVK/D1118bj8XHx6vuNekNMe39yM7OVo81xs/PT21ERETkHprV8zFv3jzExsbimmuuMR7r16+fmoKzcuVK47G0tDSkp6cjJSXFMq0lIiJyQlKq4u2337bIa61Zs0YNf8iHfbfp+ZDEIQk+Jk+eDG/v818u+Rr333+/SkCNjIxUma7Tpk1TgUdTZ7oQERE5iiuvvBKXX365RYIGKVURFBRkkXa5ArODDxlukd6M++6774LH3nrrLZVUNGnSpDpFxsj5ZReW4387TmJinzaID/O3d3OIiBxi6qnUvjL9IH6xWSLUgmGXsWPHqje8c+fOFzzm7++vKp2ePXsWJSUlKifkYvke5Dw+XHcUry1Pww3vbcCBLE6HJqLmk2tIaWW1XbamlraSmZxr167FP//5TzXEIdv8+fPV7bJly1SqgeQr/vrrrzhy5AgmTJigZndK8a0BAwaoD+oXG3bx8PDAxx9/jBtvvFFNQZa6Gd9++22z39P//e9/6NGjh2qTfK833nijzuPSESDfQ67T0k6tHpf46quvcNlllyEgIABRUVGqZIZcw62p2XU+yL2cPKevTJhVWI6b30/FB3f3w5BO0fZuFhE5obKqGnR/5ie7fO99L4xDoO+lL30SdBw8eBA9e/bECy+8oI7t3btX3T755JP4xz/+gQ4dOiAiIkLN0vzDH/6Al156SV38P/30U1x33XUq71FqXTXm+eefx2uvvYbXX38d7777Lu688041oUNSF8whdbZuueUWPPfcc7j11luxceNGteyJBBISRG3btg1/+tOf8Nlnn2HIkCGqg2D9+vXqa6Xy7O23367aIYFQUVGReqwF9UebhMEHNUlOkb4QXHSwH84UV2DyvC14/abeuKFPa3s3jYjI4iSPUWpXSa+E1oMvVbuFBCOytIhGgoXevXsb77/44otYsmSJ6smQZUYac++996oLv3j55ZfxzjvvYMuWLbj66qvNauubb76JUaNG4emnn1b3ZWRi3759KqiR7yGpEpJvcu211yIkJARt27ZFnz59jMGHrLkjS6HIcSG9INbG4IOaJKdQH3z8644++GzTCfyw6zSmf7ETp/LL8MiVHVkumYiaLMDHS/VA2Ot7t1T//v3r3C8uLla9DrK2mXYxLysrUxf9i+nVq5dxX4IDmaiRk5Njdnv279+vhn1MXXHFFWqYR3JSJFCSwEJ6aiSwkU0b7pGgSQIXCTgkT1NSK2RIRnp0rIkl56hp9fyL9cFHm4gAvHtbHzw4rL26//pPaXhq6R5U19TauZVE5Czkw4oMfdhjs8QHpfqzVv785z+rng7pvZAhi507d6qLudS+uhgfH58L3heZUWpp0tuxY8cOLFy4EK1atcIzzzyjgg6Zqisl0n/55ReVx9K9e3c1/NOlSxccO3YM1sTggy6psKwaldW1xmEXT08P/L9ruuOZa7tD/h9/vjkdD/13u0rmIiJyFTLsYrqSe2Nk5XYZ3pDeBAk6ZJhG1j+xlW7dul2werzcl+EXbf0VmZEjiaSS27Fr1y7VvlWrVhmDHukpkRyU3377Tf3cEkxZE4dd6JJyisrVbViAD/xNuizvG9oercL88dgXO7Fifw5u/2gzPpncXwUoRETOTmaNbN68WV2oZRZLY70SMotEZndKkqlcyCX3who9GI15/PHH1QwbyTWRhFNZzPVf//qXsdSFrD5/9OhRDB8+XA2n/Pjjj6p90sMhP58UB5XhFikeKvdl5VoJaKyJPR90SbmGZNPYkAuDivGXtcKCBwYhPNAHv2fkY9L7G3H8jHWnaBER2YIMp0jPgQxHSJ2OxnI4JOFTLuoyk0QCEMmd6Nu3r83a2bdvX3z55ZdYtGiRmp0jwyqSFKst/CpLnkhwNHLkSBVUzJ07Vw3ByNRcyTNZt26dmq0jPSVPPfWUmqYra7dZk4fO2vNpzFRYWKiyjGWROXlTyP6W/nZKJZcO6RiFBQ82XK32SG4xJv97C06eK0NkkC8+ntwffZOsm7BERI6vvLxc5Q+0b99e1Zgg53ax36c512/2fFCTh10a6vnQdIwJxtePDMFlrcNwtqQSd3y0CT/vzbJhK4mIyFkw+KCmD7uEXvxTS2yIPxZNGYwru8SgvKpWJaF+lmq7pCsiIlfw0EMPqRyThjZ5zBUw4ZSaXGAspgmJpEF+3vj4nv5q+u2irRl4+pu9qK7V4f+u0E/NJSKii5N8Dck3aYirpCMw+CAzej6aNovF28sTsydehoggX7y/5oiaisvgg4ioaWJjY9XmyjjsQhbt+dDIdLM7B+nXNDiRV8IiZEREZMTggy4pp7DcrJ4PTUJYAPx9PFFVo0PGuTIrtY6IiJwNgw+6qPKqGhSW6yuXxoSYN01OKqF2iA5W+0dyiq3SPiIicj4MPqhJ+R6+3p4I9Tc/RahjrCH4yGXwQUREegw+qEn5HlLjozkLMnWM0S/AxOCDiIg0DD6o2aXVm6KToefjMIddiMgN14aRZe2bwsPDA0uXLoW7YPBBF5VrqG4a08zgQyqfiiO5JXCwSv5ERGQnDD6oicMuzVuToX10EGS0pqCsCnkllRZuHREROSMGH1bwzc5TGPTyCvyWfg6uMuzS3J4Pfx8vtIkIUPuc8UJEivSCVpbYZ2tiD+yHH36IhIQEtfS8qQkTJuC+++7DkSNH1H5cXJwqey5L2q9YscJib9Hu3bvVKrQBAQGIiorClClTUFx8/m/omjVrMHDgQAQFBalVa6+44gqcOHFCPfb777/jqquuQkhIiKqI2q9fP2zbtg2OhBVOreDDdUeRXViB+RuPo4+Tr+xqmnDaXDL0knG2TA29DOoQZcHWEZFTqioFXk6wz/f+Wybgq0+Ev5ibb74Z06ZNw+rVqzFq1Ch17OzZs1i+fDl+/PFHFQjIMvQvvfQS/Pz88Omnn+K6665DWloakpL0BRabq6SkBOPGjUNKSgq2bt2KnJwcPPDAA3j00Ucxf/58VFdX44YbbsCDDz6IhQsXorKyElu2bDFOCrjzzjvRp08fvP/++/Dy8sLOnTvh4+MDR8Lgw8KyCsqxN7NQ7a86kIOqmlr4eHm6TWn1xoKPNWm5nPFCRE4jIiIC48ePx4IFC4zBx1dffYXo6GjVq+Dp6YnevXsbn//iiy9iyZIl+Pbbb1WQ0BILFixQS9dLQCM9G+Jf//qXCm5effVVFUjIsvXXXnstOnbsqB7v1q2b8evT09Pxl7/8BV27dlX3k5OT4WgYfFiYBByaovJqbD56FkOTo+GscrSE0+Dm5XzUTTpl8EFEAHwC9T0Q9vreTSQ9CNK7MGfOHNW78fnnn+O2225TgYf0fDz33HP44YcfcPr0adUbUVZWpi78LbV//34V2GiBh5BhFRkCkp6V4cOH495771W9I2PGjMHo0aNxyy23oFWrVuq5M2fOVD0ln332mXpMenG0IMVROO9Hcge16kC2upWy4uKXfVlwVjW1OpwprrRAzwdrfRCRCRkekKEPe2xm1CuSngaZpScBRkZGBtavX68CEiGrzkpPx8svv6yOy9DGZZddpoZAbGHevHlITU3FkCFD8MUXX6Bz587YtGmTekyCor179+Kaa67BqlWr0L17d9VWR8Lgw8KlyDcczlP7j17VSd3+si/baaeYniutVAGI/F+NCvJt9utoVU5PnitT7xERkTPw9/fHxIkTVY+H5FZ06dIFffv2VY9t2LBB9T7ceOONKuiIj4/H8ePHLfJ9u3XrppJGJfdDI99PelykDRrJ65g1axY2btyInj17quEajQQjM2bMwM8//6x+BglWHAmDDwtKPZqHsqoaxIf644FhHRDo64VMkxwQZ5NTqM/3kMDDuwV5K/L14YE+Ksn8aO75/0xERI5Oejqk5+Pf//63sddDy6P4+uuvVY+HBAp33HHHBTNjWvI9/f39MXnyZOzZs0clvUry6913361m1xw7dkwFHdLzITNcJMA4dOiQClpk6EdyTmQ2jDwmQYskrZrmhDgCBh8WtGq/Pt9jZLdYNcV0eHKMuv/zPv1QjNPmezSzxodGMrCZ90FEzkimu0ZGRqpcCwkwNG+++aZKSpVhDxmekfwLrVekpQIDA/HTTz+p2TUyhfemm25SSa+SdKo9fuDAAUyaNEn1cMg03KlTp+KPf/yjmt2Sl5eHe+65Rz0muSCSOPv888/DkTDh1EJkaEVLNh3VNVbdjukeh+V7s/Dz3izMHNMZ7lZavX7ex/YT5xh8EJFTkaGOzMzMBkunSz6FKQkATJkzDKOrNzwvQzn1X18jvR+N5XD4+vqqISJHx54PCzmYXYxT+WXw8/bEkI762S0ju8bCy9MDB7KKkHG2FM5a46O5BcYaK7NORETujcGHhaw0zHIZ0jEKAb5eaj8iyBcD2kU47dCLZXs+DMEHq5wSkZuRhFWpgtrQ1qNHD7gjDrtYPN8jrs7xMd3jsenoWTXl9v6h7eFOpdUbmvFy9Ewxamt18PRs+nQ3IiJndv3112PQoEENPubjYJVHbYXBhwWcK6nEDsM6LjLUYmps9zi8+P0+bD1+Tj1PekOcLeG0uYvKmUqMCICPlwfKq2qRWVCGNhFNL/RDROTMZI0V2eg8DrtYwNqDuajVAV3jQ9A6XL+ImiYxMlAdl3oZptVPnYElSqtrZKpuuyit2BjzPojcjbPWOyLr/B4ZfFjASkNQUb/Xw7T3Qys45pQJp8EtDz4E8z6I3I82rFBa6nxJ93QhrYKrTOltCQ67tFB1TS3Wphmm2HZrJPjoEY93Vh3GukO5qsKn1ABxdMUV1SitrLFYz4foJHkfe4HDnG5L5DbkIiVLvsvKrFqNCm31VXIuUkQtNzdX/Q69vVsWPjD4aCGpXVFYXo2IQB9cnqif2VJfj4RQJIT5q2qnGw6fwah6SamOPOQS7OeNQF/LnCYdYw3DLuz5IHIrUnpcaAEIOXfdk6SkpBYHkAw+WkjL47iyi76mR0PklzS6exw+TT2hhl6cIfjIKdSqm1qm10Ow1geRe5K/gbLiamxsLKqqquzdHGoBKWImAUhLmR18nDp1Ck888QSWLVumxvA6deqkFqzp37+/MRnl2WefxUcffYT8/Hy1DPD777+v6uC7Y76HZmz3eBV8rNifrZJPGwtUXLHAmKaDIfg4U1yBgtIqhAW65xQzIncegmlprgC5BrPCl3PnzqlgQhKIJPjYt28f3njjDVXfXvPaa6/hnXfewdy5c7F582YEBQWpmvfl5fpP0q4kPa8Uh3OKVSAxvLN+HZfGDOoQiRB/b7VE/c4M/bRcd6nxoZEhHFl0Txw5w6EXIiJ3ZVbw8eqrryIxMVH1dAwcOBDt27fH2LFj0bFjR2Ovx9tvv42nnnoKEyZMQK9evfDpp5+quvhLly6Fq1llqGrav20EwgIu/inex8sTV3WJdZpqpzkWrG5qinkfRERkVvDx7bffquGVm2++WY3d9enTRw2vaGSZ36ysLIwePdp4LCwsTFV2k6V/XXXIpbFZLvWN7eE8U24tWWDMFPM+iIjIrODj6NGjxvwNWe734Ycfxp/+9Cf85z//UY9L4KGtuGdK7muP1VdRUYHCwsI6mzMoqajG5qNn1f7Irk1LIB3ROUZV+TyaW6KGa9xt2KVu8OHYPz8RETlI8CFzfPv27YuXX35Z9XpMmTIFDz74oMrvaK7Zs2er3hFtk2EdZ/Dr4TOorKlF26hAtVx8U4T4+yDFsOKto/d+WHJROVcKPmTl4qvfXod//JRm76YQEblH8CFTpbp3717nWLdu3ZCenl5nLnd2dt0Lq9zXHqtv1qxZKCgoMG4ZGRlwpoXkJI/DnPnOWrXTn/c13BPkcDkfFiowVqfQGIATeaWorK6Fs3l12QEcyCrC3LVHjENTRERkxeBDZrqkpdX9xHfw4EG0bdtW7UsCqgQZK1euND4uwygy6yUlJaXB1/Tz80NoaGidzdHJqqyrLlHVtDFjDMHHzox8h714VdXU4mxJpUVLq2viQv0Q5Oulphunn3WuvI/fM/Lx7e+Zar+6VocvtzpHoExE5NTBx4wZM7Bp0yY17HL48GEsWLAAH374IaZOnaoelx6A6dOn4+9//7tKTt29ezfuueceJCQk4IYbboCr2JNZoIYl5CI6sH2kWV8bF+qP3m3CIGvzrDT0njgaqcMhvD09EBFo2VV45RzpaOj9OJzjPMGHzOR66cf9al9bPHDB5nQVRBERkRWDjwEDBmDJkiVYuHAhevbsiRdffFFNrb3zzjuNz/nrX/+KadOmqXwQeX5xcTGWL18Of3/LzppwhKqmw5Jj4OdtfsEcWetF/Lw3y+GTTT2tUAzNGnkfK/Zl429LdqOgzDrVE1fsz8GWY2fh5+2J/z4wSJXTl3L5zrZSMRGRIzC7Ruq1116rejSkaNj+/ftVwmn9T7YvvPCCmt0iz1mxYgU6d+4MV7KqiVVNLzX0suFInpo142hyCq0z00WjJehaKviQXolnvtmjeiKe/24vrDEMNXuZvtfjvqHt0T46CDf31ydG/3fTCYt/PyIiV9fyAu1uRtY82XWyQO1f2fXiVU0bkxwbrGbJSMLluoO5cJcCY9aq9SHTlqUXQny945TqBbGkRVsz1PToyCBfPHylvqDeHQOT1K2sVCyVbomIqOkYfJhpTZo+WJC8jeYW4JLeofOzXrLdpsaHRsv5OJpTrHotWmqtIYCTGipi1pLdyC/VJ8y2VFF5Ff654qDaf2xUMkL99ZVs20UHqZL60vzPt7D3g4jIHAw+zLTSUFK9qYXFGjOme7xxCEe69R2JNgsnxsLVTTXS6yOpJEUV1cZAxxLBx/TRndWQjrzmc99aZvjlg7VH1Xo8MtRyxyB9b4fmLsP9xdtOoqK6xiLfj4jIHTD4MINcYNYfOtOifA9Nv7YRqhtfEiS3HtdXSnWXYRdJ0k2KDFT7h1uY91FWWYPNx/Tv37gecfjHzb1VYLN0ZyZ+amFCb1ZBOT7+9ajaf+Lqrmp9HlNyDrQK81fTkpftdszkYSIiR8TgwwxSTr20skZdlHsktKweiayEO8oQwPy8N9uthl1Mi421dIG5zcfyVO5MQpi/yiXpkxSBKcP1eRn/b8kenDPUK2mON35OQ3lVrVo4UAKb+ry9PHG7IfeDiadERE3H4KOZs1wsMQVVm/UipdYtkfvg6KXVrZF0qg25jOgSY6w0O310skrqlXolzzZz+GVfZiG+2nFS7f/tmm6NVrG9bUCiqoey7cQ5HMhyjnWJiIjsjcFHE0lwcD7fo2VDLhqpE+Lv46nWC9l3utBhfk5j8BFqvdoslqr1oc0WGp58fuaRv4+XGn6R3iWpSLp8z2mzX1em1ko8eE2vVuibFNHo8+Q90lYrZu8HEVHTMPhoIrlIZpwtg6+XJ67opF8crqUCfL1UAOJIC81JDoosmCeigy1b3dRUx9igFg+7nDxXqnpOJMgYUu930jsxHA+N6GAcfskzVG1tam+K5PbI7JknxnW95PPvGqRfXmDJjlModsC6LUREjobBRxNppdAHd4xCkJ+3xV7XdOjFkZJNwwN9mlW9tak6ROt7PqQ+R3MLra07qE/+7ZMYjrAA/RRYU38alYwucSHIK6nEM00cfpFy6bMNZdTvHtwOSVH6xNiLSekYhQ4xQSiprMHS306Z/XMQEbkbBh9m5ntoSaKWIq8n6SN7MwvVJ3mHSTa18IJy9UUE+SIqSN+zcuxMScuGXDo3XOxNgidt+OWHXafVdin/23FSrVob6u+NaSM7Nakdkg9yp6H3Q4ZeHCl/h4jIETH4aIKC0iqVUGjJfA9NVLAf+rfVL05n6cqcLanxERtq3eCjpXkfUhtlw2F9z8eIRoIPcVmbMEw1VCV9+ps9xkXzGpu2KzNcxKMjO6kAqalu6ttG5e9I4LIjXX+uEBFRwxh8NMHaQ7mqO15mUCQa6lNYknHoZX+2w6zr0tzqrbbK+9iZka+KlMkCbz1bh130uY+OTEbX+BBVj+PppXsa7Zn4eP1RZBdWqFVr70lpZ1Z7wgJ9cF2vBLX/303pZn0tEZG7YfDRBKu1KbbdLNvrUT/42HT0rOplcfUaH/V7PppTaGytocy9JOzKsMrF+Hp7quEXmRK7bE8Wvm9g+EV+7rlrj6j9v17dRc2YMdddg/VDLzK8Y06CKxGRu2HwcQnS47E6Tcv3aFlJ9cbIOiGd44LrfC9XrW7a0BovR3LMz/mQBd0ulu9Rn/SOTL1Kn8MhK+DWL+v+z5UHVcJorzZhxh4Mc8kMm8tah6nZQou362uEEBHRhRh8XMJv6eeQX1qlZlP0TQq32vfRej9e+nE//rPxOMqrauy8rov1g49Ohp4PSTiVwKuppFdh9yn9ysLDk5s+7VmCj+6tQnGutApPLd1tHH6RVXEXbslQ+3/7Q7cWFZC7a7C+4umCzemoNeNnIiJyJww+LmGlYchFkhqlnLa1SJd9m4gA9YlcqnKOeH015m84ZvMgxJbDLgnhAfDz9lQ9BebM9Pn18BlVAKxbq1CzCqGZDr/8tDdbFSATryw7oIKf0d1iMbhDFFriut4JCPH3RvrZUmPvDBER1cXgw4Rc6LefOIt5G45hxhc7MfKNNXh/jT4PYJSV8j00rcICsPLxEXjxhp5qnRJJfHzuu30Y/tpq1R5bBSHnh12sn3AquRqyWqy5M160fI+LzXJpTPeEUFX/Q0iQ993vmVixP1u15cnxly4odimBvt64qV8btc/EUyKihlmuWpaTkamaaVlF2HWyALtO5uP3kwU4mF3UYPe/LCI3qpt18j3q16W4e3Bb3NK/jVqmfc7qw6oI1/Pf7cOcNUfw8IiOaln35iRDNoUEOEXl1Tabaqvlfcj0VMn7GNmEa78MZawzrCw8vHPzKs0+fGVH/LwvC3tOFWLawt+Ma7R0ig2BJUjNj3kbjmPVgWxVOl9mzxARkRsGH7K6qRQK0wINWUtFVkOtLzrYD73bhKFXm3D0SgxDr9ZhqhaHLUkQIsMwN/dvg6+2SxByRF3EXvh+H95fewQPjeiIO60QhGhDLjIUEmLBKq6WrPUhvzep1RHo62Wsj2IuHy/98Mt17/6Kqhodgny9MH10Z1hyxd6UDlFIPZqHRVvS8fjYLhZ7bSIiV+A2wcfJc2V4fPHvdY5JFUsVZBiCjd6JYYgP9W90BVNbkyBEPkXf3C9RVd7816rDKgh5UYKQNRKEdFCPyxoxli4wZqv3oGOMecMuWh7FkI5RKoejubrGh+KJq7vi7z/sx4wxnS2e4yLBowo+tmaoYR4JeIiIyM2Cjy7xIRjcIRI9EiTQCEPvNuFoGxXoMIHGxchF9vaBSZjUtw2+liBk9WEVTMmFc+7ao3ju+u64tpnTQ+1RWr3hno8Si5RUN8cDwzqo/IzwQMsvoCcr3UpAI+/pz3uz1eq4RESk5zYfx+QCvmhKCp6+tjsmXN5a1dZwhsCj/s9w28AkrP7zlXh10mVqdowMQcz88neLJKTaMtlUIwuyCak+KtvFyIqx246fa3ayaUOsEXgI6emQPBJtvRciInLD4MOVyIXt1gH6IEQWZ5PcFUnatFhpdRslm2qzQ7SEzEsNvaQeyUN1rU71WLWN0gctjkx6q6RkiAy/SC0RIiLSY/Dh5EGItq6JVnTL2YZd6lY6vfgFeu3B8zVXnIHUMRlpqIr7+Wb2fhARaRh8ODkp5y32WiD4sOWKtuYmnUo10rVavkeycwQfphVP/7f9pFo1l4iIGHw4vZ6tQy3W82GPnI+mJp0ezytFxtky+Hh5IKVjy6qQ2pIESkmRgSgsr1YFzYiIyI1mu7gqbdhFCqRVVNeo6bnOUFrd3Fof2iwXqe0RZKMaJJYg68RIYTgp4f72ioNYf1hfIE1bV6ZOSTvDHZ3J0aggP0wb2cmsMvJERI7Oef6KU4MkWTMi0EctliYVW6VeSXNIZVeZOWOrFW1NdYzVD7tknC1Vs3YaKp6mDbmM6OI8Qy6aW/onqsBDqtVmNqP3Q6qxfnRP/2b/bomIHA2DDycn04Wl92P9oTNq6KW5F6i8kgpIZXmZfRwZZJ3pp42RBFdZjE1Ku5/IK1U1WUxJj47MdHG2fA+NvJ8LHxyMHen50CZ3a7O8z9/3uOC49H98mnpCzZS5eW4qXrupl5omTkTk7Bh8uAAt+JC1Slo65CLd/NZcvbchcuGVoZedGflq6KV+8CG1PcqqatRwULdWlll/xdb6JEWozVw39mmNxxbtVEsDyK0Mrz0+posaziEiclZMOHWhGS97WpB0ej7Z1LZDLqbroYiG6mEYq5omxzhdYbiWCvH3UUMufxzRQd1/b/URTPlsuyq4RkTkrBh8uFDwITkfDS2W58jJpk1JOnXmfA9L8PL0wKzx3fDmLb1VldsV+7Mxac5GlSNDROSMGHy4ACmzHhbgg8qaWtUt35Lgw149H43V+sguLFfVW6XDY1inaLiziX3b4Ispg1WAmJZdhOv/9Ss2HdXnwhARORMGHy5AhiJaOvSSU2ifAmMXVjktQa1kvtYbcpFE2ggbJ8I6Iskb+fbRK9TvW2Y43fXxZlZPJSKnw+DDRfRoYbGx3GL7lFbXSCEub08PlViaZQiE6gy5JLt3r4epVmEBWPxQCq7rnaDWuvl/S/bgmW/2oKqmeUNuRES2xuDDRbS850NbVM7fbuvUyIJxpkMvUnvkV0NRruFOsp6LrUgtlHduuxx/GddF3ZcpuZP/vQXnLrEyMBGRI2Dw4WLBx/6somZ9Arb3bJc6SaeGGS+7TuYjv7RK1QC5PJEFthoabpt6VSd8eHc/BPp6YeORPNwwZwMONTPvh4jIVhh8uAgZtpCLtMx2MTfpVEp923u2S528D8MaL+sO6ns9hnaKtnntEWcytkc8vn5kiEo8liJtN87ZiJd+2IdVB7JRVF5l7+YREV2ARcZcqdJpQhhSj+Zh76lC9EjQ94Q0hdSMkFwLuwcf9abbrj2Yo25HcMjlkrrGh+KbqVfg4c93YMuxs/ho/TG1SS0y6RUb3DEKKR2iMKCdc62NQ0SuyayPk88995y6yJluXbt2NT5eXl6OqVOnIioqCsHBwZg0aRKys7Ot0W5qwGVtwpqVdKoNuQT7eSPQ134XJtNCYwWlVariqWC+R9NEBfvh8wcG4Z3b++C2AYkqh0YmDv1+sgAfrD2Ke+dtRe/nf8bEORvw2vIDWH8oF2WV+qCTiMiWzL7S9OjRAytWrDj/At7nX2LGjBn44YcfsHjxYoSFheHRRx/FxIkTsWHDBsu1mC65wq25wYe9a3xoOhhqfUgwtHzvaXXhTI4NRkJ4gF3b5Uwkcff63glqE5n5ZWpdHOkRk9tT+WVqjRnZ5qw5Ah8vD5VPI70iUkekXbT+d0BE5FDBhwQb8fHxFxwvKCjAJ598ggULFmDkyJHq2Lx589CtWzds2rQJgwcPtkyL6dJJp6cLUV1T2+Q8Ca3nw55DLiLU30cFQNKeeRuOq2Ps9WgZCdwm9WujNiFVUSUQ2WQISE4XlGPr8XNq+37Xaax8fITblbAnIicIPg4dOoSEhAT4+/sjJSUFs2fPRlJSErZv346qqiqMHj3a+FwZkpHHUlNTGw0+Kioq1KYpLGz+4mjurm1koBo6kRyOw7nFKg/AnAJj9g4+tLwPCT6kqqlgvodlJUYGqu2W/okq0ThdgpEjeXjmm704eqZE5dt0inXOxfuIyEVzPgYNGoT58+dj+fLleP/993Hs2DEMGzYMRUVFyMrKgq+vL8LD606JjIuLU481RoIXGaLRtsTExOb/NG5OVjrtkWAoNnaywOwCY7Eh9qnxYapj7Plufz9vTwxsH2nX9rgy6eFoGxWE2wYmYVAH/fssq+cSETlU8DF+/HjcfPPN6NWrF8aNG4cff/wR+fn5+PLLL5vdgFmzZqkhG23LyMho9mtR84qN5RoLjDlGz4dmcIcoVUyLrG9k11h1y+CDiGyhRcUTpJejc+fOOHz4sMoDqaysVMGIKZnt0lCOiMbPzw+hoaF1NrLtjBd7l1ZvLPhgvoftg49tx8+hoIy1QYjIgYOP4uJiHDlyBK1atUK/fv3g4+ODlStXGh9PS0tDenq6yg0h29Dqe+w7XajKk5tXWt0Bgg/DdFvBfA/bkeEXWVlY1oqRKbhERA6TcPrnP/8Z1113Hdq2bYvMzEw8++yz8PLywu23367yNe6//37MnDkTkZGRqgdj2rRpKvDgTBfb6RAdhCBfL5RU1qjkwc5xl04ezClynITThDB/3DukndqXiyHZtvfjSO4xNfRybS/9VF0iIrsHHydPnlSBRl5eHmJiYjB06FA1jVb2xVtvvQVPT09VXExmsEheyJw5c6zScLpY0mkYthw/q5JOLxV8SDl2WZrdURJOJQnyuet72LsZbumqrrGqKuratFzU1urUuUREZPfgY9GiRRd9XKbfvvfee2oj+xYbU8HHqQJjfYfGnDHke0ixqfAAHxu1kByRlF4P8fNGXkklfj+Zjz5JEfZuEhG5KK7W5YJ6tg5t8owXrcBYdLAfP+m6OamOOqxztNpfzVkvRGRFDD5ceLptU5JOHaW0OjmGkV3j1O1KBh9EZEUMPlxQh5hgBPp6obSyBsfO6FeIvXSyqf3zPcj+ruwSA6muvjezENmGyrdERJbG4MMFeXl6oHsrQ6XTSwy9aNNsHWGmC9mfDL/1aqOvUsyhFyKyFgYfrr7C7cnCJpZWZ/BBeiO7sNopEVkXgw8XDz72ZBY4TYExcqxqp78ePoOK6hp7N4eIXBCDD1dPOs0sVDUbnKG0OjkGWZxQesIkZ2jLsbP2bg4RuSAGHy5KqoP6+3iiuKIax/JKGn1eriGpMDaUCaekJ1OurzIMvazcz6EXIrI8Bh8uytvL05h02li9D51Od77ngzkfVK/aqVidlqPOEyIiS2Lw4RZJpw0HH/mlVaiq0V9YOOxCpoYmR6uqtyfySnH0TOM9Z0REzcHgw42TTrXqphGBPvD15qlA5wX7eWNQ+yi1zym3RGRpvOK4QdLp3lMNJ5060mq25LizXjjllogsjcGHC0uODYaftyeKKqpx4mzpRUqrM9mUGg8+ZMZLYbl+5WMiIktg8OHiSaddL1LpVBt2YYExaki76CB0iA5Cda0Ovx46Y+/mEJELYfDh4i4zrHC7t6Hgg6XVqYmzXjj0QkSWxODDTfI+Gur54DRbaurQy5q0nIsWqyMiMgeDD3eZ8XKq4IJ6DTksMEaXMKBdpJr5cqa48pKLFBIRNRWDDxeXHBsCXy9PFJZXI71e0ilLq9OlyBTsYcnRap9DL0RkKQw+3ODi0bVViNrfc6ruCre5XFSOmoB5H0RkaQw+3KnSqUm3eVlljZqCK5jzQRdzZZcY4/mjDdUREbUEgw83Sjo1XeNFq/Ehi8+F+HnbrW3k+KQOTK82+nNoTVquvZtDRC6AwYcb6JlwvudDSzrVqpvKhcXDw8Ou7SPHx2qnRGRJDD7cQOf4YLVIWEFZFU6eK6tTYIxDLmRO8LH+UC4qq2vt3RwicnIMPtyAn7cXusSH1Bl6OV9ancEHNa33LDrYDyWVNarcOhFRSzD4cNNiY+eHXRh80KV5enrgKkPiKYdeiKilGHy4iR4meR+CpdWpuUMvq9MYfBBRyzD4cMMZL5J0qhUY44q21FRDk6NV7tCxMyVqIyJqLgYfbkJyPrw9PXCutAqZBeXs+SCzhfj7YGD7SLXPoRciagkGH27C38cLneP0Sae7TxZwUTlqlqu6GIZeGHwQUQsw+HAjPVuHqtvfT+YjTxt2YWl1akbex+ZjeSg2VMglIjIXgw83zPtYm5YLWR3d0wOICmLwQU3XISYY7aICUVWjw6+HWO2UiJqHwYcbrvGy77R+gbmoYD94SQRCZAYuNEdELcXgw410axVaJ9hgjQ9qjlFd49TtaulBky40IiIzMfhws6TT5Nhg430mm1JzyIyXIF8vVSV3b6a+F42IyBwMPtx06EWw54Oaw9fbU9X8ECsPZNu7OUTkhBh8uGnSqWCBMWpxtVPmfRBRMzD4cOOeDw67UEvrffx+sgCn8vUrJRMRNRWDDzfTvVWommIrOOxCzRUb6o8hHaPU/ifrj9m7OUTkZBh8uJkAXy/0TgxX+x1Nkk+JzPXHER3V7aKt6cgvrbR3c4jIXYKPV155BR4eHpg+fbrxWHl5OaZOnYqoqCgEBwdj0qRJyM5mUpoj+eDufvjfw0OM5daJmmN4crTqSSutrMGnqSfs3RwicofgY+vWrfjggw/Qq1evOsdnzJiB7777DosXL8batWuRmZmJiRMnWqKtZCGSaNqvbYS9m0FOTj54/HFEB7U/f+NxlFXW2LtJROTKwUdxcTHuvPNOfPTRR4iIOH8RKygowCeffII333wTI0eORL9+/TBv3jxs3LgRmzZtsmS7icgBXHNZKyRGBuBsSSUWb8+wd3OIyJWDDxlWueaaazB69Og6x7dv346qqqo6x7t27YqkpCSkpqY2+FoVFRUoLCyssxGRc/D28sSUYfrejw/XHUV1Ta29m0RErhh8LFq0CDt27MDs2bMveCwrKwu+vr4ID9cnNGri4uLUYw2R1wkLCzNuiYmJ5jaJiOzo5v6JiAryxclzZfhh92l7N4eIXC34yMjIwGOPPYbPP/8c/v6WKVA1a9YsNVyjbfI9iMi5yvbfO6Sd2p+79ih0Oq73QkQWDD5kWCUnJwd9+/aFt7e32iSp9J133lH70sNRWVmJ/Pz8Ol8ns13i4+MbfE0/Pz+EhobW2YjIudyd0haBvl7Yf7oQaw/m2rs5RORKwceoUaOwe/du7Ny507j1799fJZ9q+z4+Pli5cqXxa9LS0pCeno6UlBRrtJ+IHEB4oC9uH5ik9ueuPWLv5hCRg/M258khISHo2bNnnWNBQUGqpod2/P7778fMmTMRGRmpejGmTZumAo/BgwdbtuVE5FDuH9oe/9l4HJuOnsXOjHxcbihmR0Rk9Qqnb731Fq699lpVXGz48OFquOXrr7+29LchIgeTEB6ACZe3Vvtz17D3g4ga56FzsOwwmWors14k+ZT5H0TO5VB2Eca8tQ4eHsCKmSPQMYYl/IncRaEZ12+u7UJEFpMcF4LR3WIhH2k+WnfU3s0hIgfF4IOILOohw4JzX+84hZzCcns3h4gcEIMPIrKo/u0i0b9tBCpravHJhmP2bg4ROSAGH0Rktd6PBZvSUVheZe/mEJGDYfBBRBY3smsskmODUVRRjc83pdu7OUTkYBh8EJHFeXp6GHs/Pvn1GMqrauzdJCJyIAw+iMgqrr88AQlh/jhTXKGST4mINAw+iMgqfLw8cf+wDmr/w3VHUFPrUCWFiMiOGHwQkdXcNiARYQE+OJ5Xip/2Ztm7OUTkIBh8EJHVBPl5Y3JKW+OCcw5WUJmI7ITBBxFZ1eQh7eDv44ldJwuQeiTP3s0hIgfA4IOIrCoq2A+39E9U+++v5YJzRMTgg4hs4MFhHeDl6YH1h85gz6kCezeHiOyMwQcRWV1iZCCuuayV2v+AC84RuT0GH0RkE38coZ92+8OuTHyw9gjSsoqYgErkpjx0Dva/v7CwEGFhYSgoKEBoaKi9m0NEFvTAf7Zixf4c4/34UH8M7xyN4Z1jMLRTNMIDfe3aPiKyzfWbwQcR2UxZZQ0WbknHukO52HQ0D+VVtcbHPD2A3onhGJ4cgxFdYtC7TbjKEyEi58Dgg4gcnqz3svX4WaxNy1XByMHs4jqPS3Ey6Q0Z0TlG9YzEh/nbra1EdGkMPojI6ZwuKMO6g7lYd/AM1h/KRWF5dZ3HY0P80DkuxLAFI9lwG+LvY7c2E9F5DD6IyKlV19Ti95MFKhhZezAXv5/MR2N/qWTxOi0Q0YKT5LhgBPp627rZRG6NwQcRuZTiimocyi7CoexipGUX4aBhyy6saPRrEiMD0Do8AOEBvggP9EFYoA8iAn0RHuCjvx/gi4ggH+Pj/j5eNv2ZiFyNOddvfjQgIocX7OeNPkkRajNVUFqFQzlFKiCRwEQLSs4UVyLjbJnamsrP21MFJ5cnhuO1m3shlMM5RFbD4IOInJb0ZvRvF6k2U2dLKlUQklNUgYLSSuSXViG/rArnSitVwFJ/v6ZWh4rqWmQVlmP53izU6nT44O5+8PDgbBsia2DwQUQuJzLIF4M7RDXpuTLyLMM6EqBI4bNHPt+Bn/dlq0qsD43oaPW2ErkjVjglIrcmvRsyY0ZKwI/uHodnr++ujr+2/AA2Hjlj7+YRuSQGH0REJu4YmIRJfdugVgf8aeFvyCoot3eTiFwOgw8iono9IX+/oSe6tQpViauPfL4dldXnK7ESUcsx+CAiqifA1wtz7+qLEH9v7EjPx8s/7rd3k4hcCoMPIqIGtI0Kwtu3Xq725288jm92nrJ3k8ikNH9OIYfDnBmDDyKiRozqFodHr+qk9p/83241G4bsb8pn2zH01dXYc6rA3k2hZmLwQUR0ETPGdMaw5GiUVdXg4f9uR1F5lb2b5NZ+Sz+nyu5X1tTik1+P2bs51EwMPoiILsLL0wP/vK2PWkPm6JkS/GXxLlUbhOzDNOD4YddpnCluvMQ+OS4GH0RETShaNueufvD18lQVUD9cd9TeTXJLJ8+VYtmeLLUv6/ZI78cXWzPs3SxqBgYfRERNIGu+PHOdvgDZq8sPIPVInr2b5HbmbziuSuFf0SkKM8d0VscWbE5Xx8i5MPggImqiOwclYWLf1qoA2bSFvyGbMy5sRnJtFhl6OR4Y1gHX9GqFiEAfnMovw8r92fZuHpmJwQcRkRkFyF664TJ0jQ9RuQZTP9+BqhoWILMFGV6RNXg6xQZjRHIM/H28cMuARPXYZ5tO2Lt5ZCYGH0REZhcg66cKkG07cY4FyGyguqYW8zYcV/v3D20PT0/9asN3DWoLWXh4/aEzOJpbbOdWkjkYfBARmalddBDevEVfgEwuit/+nmnvJrm0n/Zmq+EVSfy9sU9r43FZDHBkl1i1z94PFw4+3n//ffTq1QuhoaFqS0lJwbJly4yPl5eXY+rUqYiKikJwcDAmTZqE7GyOxRGR6xnTPQ6PXNlR7T/5v114ZdkBrE7LUUMDZFkf/6qfXXTX4LZquMXU3Slt1e1X20+itJLvvUsGH23atMErr7yC7du3Y9u2bRg5ciQmTJiAvXv3qsdnzJiB7777DosXL8batWuRmZmJiRMnWqvtRER29fjYLhjaKRqllTWYu/YI/m/eVvR+/mdMeG8DZi/br4IRFiVrme0nzuG39Hw1zfnuwfpAw9Tw5Bi0iwpEUXk1lv7GHihn4aFrYbWcyMhIvP7667jpppsQExODBQsWqH1x4MABdOvWDampqRg8eHCTXq+wsBBhYWEoKChQvStERI6sorpGFbvadDQPm46eRfrZ0jqPS3rCZa3DMLhDlNr6t4tAiL+P3drrbGRV4R93Z+GW/m3w2k29G3zOx+uP4u8/7FeJwMseG6YSg8n2zLl+ezf3m9TU1KgejpKSEjX8Ir0hVVVVGD16tPE5Xbt2RVJSklnBBxGRM/Hz9sLEvm3UJiQ3YfPRPGw+ehabjuXhRF4pfj9ZoLYP1h1VwUhPQzAyrkcc+iZF8GLZiIyzpVhuKCp2/9AOjT7v5n6J+MfPaTiQVaSSgAe0i7RhK6k5zA4+du/erYINye+QvI4lS5age/fu2LlzJ3x9fREeHl7n+XFxccjK0p88DamoqFCbaeREROSspPKmaTCSKcHIMUMwcjQPx/NKsetkgdqkUmqH6CBM6tcGk/q2QXyYv72b71AkmVdqqsjaOl3iQxp9XligDyb0bo0vtmXgs9QTDD5cMfjo0qWLCjSkW+Wrr77C5MmTVX5Hc82ePRvPP/98s7+eiMiRJYQH4MY+bdQmThdIz8hZtTialAqX9WJe/ykNb/ychmHJMbipXxuVzFo/sdLdFJZX4Yut6caiYpciiacSfCzbcxq5Rd0RE+Jng1aS3XI+ZJilY8eOuPXWWzFq1CicO3euTu9H27ZtMX36dJWM2tSej8TEROZ8EJHLk5kxP+4+ja+2ncSW42eNx0P9vXH95QlqOKFXmzC3HJb5aN1RvPTjfnSOC8ZP04c36T2YOGcDdqTn4/ExnTFtVLJN2knNy/locZ2P2tpaFTz069cPPj4+WLlypfGxtLQ0pKenq2Gaxvj5+Rmn7mobEZE7CPbzxi39E/HlQylY8+crMW1kJ7V6bmF5Nf67KV3Nmhn39jp8uO4Icorcp5S7VI2dt0G/eu0DQzs0Ofi6J6Wdul2wJV0VJiMX6fmYNWsWxo8fr5JIi4qK1MyWV199FT/99BPGjBmDhx9+GD/++CPmz5+vgohp06apr9u4cWOTG8TZLkTkzmSRNFm0bvH2DJVsWVGtv4h6eXrgys4xuKFPa1zZJcalZ8xI0bY/LfwN0cG++PWJkU0egpKZR0Nmr0JeSSXm3tUXV/dsZfW2kg1mu+Tk5OCee+7B6dOn1TeQgmNa4CHeeusteHp6quJi0hsybtw4zJkzx5xvQUTk1iTIGJocrTbJe/j+99MqEJFaFysP5KjNx8tDzZYZ2z0Oo7vHoVVYAFyFfB6WqbPi7sHtzMp9kZlHtw5IxJw1R/Bp6gkGH66c82Fp7PkgIrrQ4Zxi/G/HSfxkSFI1JXVEJElVNql14cw5IluPn8XNc1Ph6+2J1CdHIirYvMRRmeo87NVVapbMipnD0Sm28VkyZL/rN4MPIiInDER+2ZeNX/Zl4beMfJj+FU+MDMDobvpAZGC7SHh7OdcSXn/8bJtay+X2gYmYPbFXs17jwU+3qfdnckpbPD+hp8XbSA1j8EFE5CZyiyqwcr8EItn49fAZY46ICAvwwciusSoYGdY5GqEOnidyIq8EV/5jjQqmfpkxHMlxzeu1WH8oF3d/skUl9G762yh1Sy5S4ZSIiOxP6lncNjBJbbKw2rqDZ1QgsupANs6VVmHJb6fUJrkk/ZIiMKJLjEpY7d4q1OGGZ6SomAQe0r7mBh7iio7RqnibDE/Jz97QmjBkX+z5ICJyQTLVVBZlU4FIWg6O5tbNE4kN8VMX+Su7xOKKTtGql8SeCkqrkPLKSrVI33/vH6QSblvi378ewwvf7zOrTgi1DIddiIjognVS1qTlYE1aLjYeyUNZVY3xMUfoFZFVgV9ZdsBii8MVlFVh8Msr1c+5aMpgNTuIrIvBBxERNaq8qkbNKll9IBdrDjbcKzKkY5SqJSIL4UkgILGAp4dHg/flVkIFT08P9EwIUz0pAb5eZhUVG/bqamQVluMfN/dWJeYtYdbXu7FwSzqu6dUK793R1yKvSY1jzgcRETVKamfIOjKyPYPuF/SK5BRVYOnOzBa8vieGdorBmO6xGNk17pLrrEiJeQk85HnX9bZcbY57Utqq4EOmJ+cUliM2lAv3OQoGH0REbi4xMhB3p7RTm9YrsjM9H1VSLEOnUzUzag230lku+9Jnrh3XH4Ma4pDqrFJrY8X+bLV5eOzG5Ynhxum/ybHBdYZU5Gs/MhQVk6mxUijMUrq1CsWAdhHYevycKrk+fXRni702tQyHXYiIyGLkkrL/dJEx+Nh1sqDO40mRgSoQGd09FgPaRaqk2Ns+3KR6SzY+OQqRQb5WKdUuQ0kbnhwJHyere+JMOOxCRER2Ib0a3RNC1fanUcnIKijHygPZWLEvGxuO5CH9bCn+veGY2mT1Xm2Nmkl921g88BBX94hHdLCfGkr6eW+2yv8g+2PwQUREVhMf5o87B7VVW0lFNdYfOqN6RFYdyMHZkkq1gq+4b2h7q3x/KdMu1VLfXXUYn6YeZ/DhIBh8EBGRTQT5eePqnvFqk9V7f0s/p5JcO8QEoWNMsNW+7x2DktRic5uPnVXDMCkdoi6ZBEvWxZwPIiJyeQ99th3L92YZ78eH+qNHQih6tA5Dz4RQ9GwdhlZh/ixG1gLM+SAiIjLxtz90U7VHdp3MV2XXZWqvbCsP5BifIzknPQyBiNQr6dk6VCXIMiCxPPZ8EBGRW5Hck/2nC7HnVAH2ZOpvD+UUq6Gg+kL8vBEWaH7p+fBAH3SOC1EVW+W2S3yI6m1x5UCmkBVOiYiImk7qm6RlFWFPZgH2nCrE3swCHDhdhMqa86sEt5TM7uliEox0NgQn4YGWn+VjDww+iIiIWkjKvkvpeVkt2BxyUc0tqsDBrCIcyC5StzLU01DPipAaJBKMtI8OUkm5AT5eCPT1UpVo5Vbuy5BRoK+3cV/dN+zL8xwBgw8iIiIHUlFdg2NnSlTvSlpWEQ5mFyEtuwgZZ8ta/NrBft6IDfVTQUxsiL/+NrTefqi/GkKy5rAPE06JiIgciJSN7xofqjZTxRXVOCS9I9lFqgBbWWUtyqqqUVZZg9LKGlWyXtuXoSHTY9qQkLxGcW71BQsE1idVZLWARKY3v3ZTb9gLgw8iIiI7CfbzRp+kCLWZq7qmFiUVNcgtrkBOUbka6skp1O/nmO4XVqCoohrlVbUqwJGtoKwK9sTgg4iIyAl5e3kiLFA2H3SKvXiRNukpMQ1K7L3EDYMPIiIiFxfg64W2UUFqcwRc3o+IiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIgcN/iYPXs2BgwYgJCQEMTGxuKGG25AWlpaneeUl5dj6tSpiIqKQnBwMCZNmoTs7GxLt5uIiIjcIfhYu3atCiw2bdqEX375BVVVVRg7dixKSkqMz5kxYwa+++47LF68WD0/MzMTEydOtEbbiYiIyAl56HQ6XXO/ODc3V/WASJAxfPhwFBQUICYmBgsWLMBNN92knnPgwAF069YNqampGDx48CVfs7CwEGFhYeq1QkNDm9s0IiIisiFzrt8tyvmQbyAiIyPV7fbt21VvyOjRo43P6dq1K5KSklTw0ZCKigrVYNONiIiIXFezg4/a2lpMnz4dV1xxBXr27KmOZWVlwdfXF+Hh4XWeGxcXpx5rLI9EIiVtS0xMbG6TiIiIyJWDD8n92LNnDxYtWtSiBsyaNUv1oGhbRkZGi16PiIiIHJt3c77o0Ucfxffff49169ahTZs2xuPx8fGorKxEfn5+nd4Pme0ijzXEz89PbUREROQezOr5kNxUCTyWLFmCVatWoX379nUe79evH3x8fLBy5UrjMZmKm56ejpSUFMu1moiIiNyj50OGWmQmyzfffKNqfWh5HJKrERAQoG7vv/9+zJw5UyWhSrbrtGnTVODRlJkuRERE5PrMmmrr4eHR4PF58+bh3nvvNRYZe/zxx7Fw4UI1k2XcuHGYM2dOo8Mu9XGqLRERkfMx5/rdojof1sDgg4iIyPnYrM4HERERkbkYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiInLs4GPdunW47rrrkJCQAA8PDyxdurTO4zqdDs888wxatWqFgIAAjB49GocOHbJkm4mIiMidgo+SkhL07t0b7733XoOPv/baa3jnnXcwd+5cbN68GUFBQRg3bhzKy8st0V4iIiJyct7mfsH48ePV1hDp9Xj77bfx1FNPYcKECerYp59+iri4ONVDctttt7W8xUREROTULJrzcezYMWRlZamhFk1YWBgGDRqE1NTUBr+moqIChYWFdTYiIiJyXRYNPiTwENLTYUrua4/VN3v2bBWgaFtiYqIlm0REREQOxu6zXWbNmoWCggLjlpGRYe8mERERkbMEH/Hx8eo2Ozu7znG5rz1Wn5+fH0JDQ+tsRERE5LosGny0b99eBRkrV640HpMcDpn1kpKSYslvRURERO4y26W4uBiHDx+uk2S6c+dOREZGIikpCdOnT8ff//53JCcnq2Dk6aefVjVBbrjhBku3nYiIiNwh+Ni2bRuuuuoq4/2ZM2eq28mTJ2P+/Pn461//qmqBTJkyBfn5+Rg6dCiWL18Of39/y7aciIiInJKHTopzOBAZppFZL5J8yvwPIiIi52DO9dvus12IiIjIvTD4ICIiIpti8EFEREQ2xeCDiIiIbIrBBxEREdkUgw8iIiKyKQYfREREZFMMPoiIiMimGHwQERGRTTH4ICIiIpti8OEoamuA378ACk7auyVERERWxeDDUax/E1gyBfhsIlBdYe/WEBERWQ2DD0eQnw6sf0O/fyYN+PVte7eIiIjIahh8OIKfnwKqy4CQVvr76/8BnDlk71YRERFZBYMPezu6Btj3DeDhBdz5FdBpDFBTCXz3GFBba+/WERERWRyDD3uqqQJ+/Kt+f8ADQHxP4Jo3AJ9A4MQGYOd/7d1CIiIii2PwYU9bPtTneARGAVfN0h+LaAtc9bfzwzHFOXZtIhERkaUx+LCXomxg9Wz9/ujngICI848NehiI7wWUFwDLn7RbE4mIiKyBwYe9rHgOqCwCEvoCl99V9zEvb+D6dwAPT2DP/4BDv9irlURERBbH4MMeMrYAvy/Q7//hdcCzgV9DQh9g8CP6/e9nApUltm0jERGRlTD4sEcl0x//rN/vcxfQpn/jz71yFhCWCBSkA6tftlkTiYiIrInBh63t+BQ4/TvgFwaMeu7iz/ULBq55U7+/aQ6QudMmTSQiIrImBh+2VHoWWPmCfl9mtwTHXPprOo8FekwEdLXAd38Caqqt3kwiIiJrYvBhS6tfAsrOArHdgQEPNv3rrn4F8A/T95hs+cCaLSQiIrI6Bh+2krUb2PZv/f741/QzWpoqJA4Y86J+f9XfgXMnrNNGIiIiG2DwYQs6nb6SqQyd9LgRaD/M/NfoczeQNASoKgV+eFz/mkRERE6IwYct7P4KSN+oL5s+9u/New2Zjnvd24CXL3D4F2Dv15ZuJRERkU0w+LC2iiJ9mXQx7HEgrE3zXyumi/41xLIngLJzlmkjERGRDZmReEDNsu51oDgLiGgPDJnW8tcbOkNf9fTMQeCXZ4Dr34VLqSrTr3kj1V1lsT2fAHu3iIioeWR4XGo71VYDOsOtul9j2NeOm9w37teYfE29r6vzWobHZaHSi+7LbY3hfhUQFAuM/H92e2sYfFjTmUNA6pzzM1a8/Vr+mvIa1/0TmDdeXzOk121Auysu/XVy0hVk6NskW00F0G44kHA54OkFh3DwJ+DHvwD5hoTaLR8BV88GuvwB8PCwd+uIqDHVFUDJGaA0T/8BQv6+VFcC1eXn99WtYat/TC6IjV2cL3oRrtU/Jre1pvs1DR+XTfEw/E1p5Lb+Me1rtU29fv3vXe/7aG1wVFHJDD5cNuKVoRGJMJPHAV2uttxrtx0C9J0M7PgP8N1jwMMbzgc2FcVAniHAUNtB/e3ZI/o/BPUFRAIdRwKdRutvZWaNreWnA8tnAQe+198PSTAcPwEsugPoOAoY/yoQnWz7thG5G7mQVhYDFYWGgOIMUJIHlOQa9g1BhtzX9uW5ZB4PL8DTW//hT26lt1ft+xiOexputc3L5Gu86z3H8DUyi1K7b7pvfEzb99Gvpm7PH1+nc6xpE4WFhQgLC0NBQQFCQ0Mt98Laj2mrT9AHftBfOCVB9JFNQFRHy76+5Hv8ayBQkgO0H6E/JkFGUWbjXyNtieyov4hLZH5s3YV/NOIv0wcisiUO0p+k1iKffFL/Bax9Dagu0/+nkPVsRjyhf3z9G/rHayr1/2lSHgGG/wXwC7Fem8jxyCdIyZ3y9gd8/B2kPYX6NmmbXKyNPBr4W1P/mHbfcMGRW7mwqPue9e5rjxs29ala606v0hceVLcm9+X/jOlj8sFDPpjUabPJvtqKzx9vDmmvXNB8A/W/K/l7Ix+KvPwAb1/DrWG74DHfehfiehdm423955i+dx7nL9DG9830PTU8x3g90DVwqx409F6YPGZ8PY8Gfi/arceFx+Tvp3puvZ/Bw/B8F2PO9dt9go+Ck8A7fQD/cCAgXH8rhbu0/QZvw/T7vkEXdsOJOsdwfl/+488dpv/kPnQmMPpZWMWer4Gv/u/C40Ex+i41CTKiOxtuk4HwtnWHWOSP1cltwOEV+u10vfLtviFAhxHne0Yi2lqu7RL4yJRh6ZkRba8ArnkDiO1W93l5R4DlTwKHftbfD2mlr3ly2U0u+Z/XpcmfGlkgUQrtSbVf4+05/aYdM92X2/IC/UVAyMXKPxTwC613G9bwcbkINDT2rV2UGxofl2EAFVyYBBjlJvtVbrLIo1wkJZgIjAaCDJvajwGCTI/H6J8nfysbWiST3EYhg48GZO0B5jYhN8KSQlsDj27VBy/WIL+6rR/rAystyIjqBARGNu/1inOBo6sNwchKfRerKXntdsOAdkP1wUJoK/O/R1GWfvbP7sX6+/KHS6Yf97r14sFE2nJg+RPAueP6+1Lz5A+v6XtqyPaq5JN0IVCW30AwYXp7ru59CcxdhXy6l1442Xy0DyiGP6d1/qpqx3QX3q+TN1BjOGZ63/C4aU6B9klaPlVr3euq50Dravep+5jcSi+DBGSqvcHn2y0fMLR9ddzwHN9gfbI3A3wyA4OPhsgnmqLT+j+W5bIVnN+/2DG5lU9I5pI/Drd8CnS9Bk5J/thl7TofiGRsvjB5KrKDPv+k7VB90mt4UuOvJ580JVCSEvNqqMdDP5tl5FP6XqamXvBS3wXWvaEfppE/wv3vB676W/MDLlcjFyzpXZDzXS70qvvd0A2vjhmOq0//9Y7J0IHxE36hYV9uC+rel8dbEkTIhVJyjeR3pm4jgICIesci6x6TXkgZOihvoD0X3Dc5Jhfs+uPf6sJsuH/Bvlyofev2oBgvzqF1b+V5RGTE4MMaU6UuGB9Ew2OG2tfIH1hHGJ+2FPljfvxX4MRG/a0EJsbMcYOwRH2PiAQicivBiXxyytgK/DBDX2JeJPQFrn0TSOjTvLbkZ+h7T/Yt1d+XC5QMbUkVWG1YSX4HUg22zsWqoN6+4UKqjeOajq0bk7u02wbG4I1MPh3WH9Ovc8z0/iWy7bXX0GnJf4YEQOO+Ic/A9FaO23pIQIY7AhsKHEwCiPrHjcOYRORKGHyQ9cnFO30zcGKDfsv8Tf8J21RwPBDTWZ/fIWRMWIIEmaljiem9R9cCy/4K5B7Q3w9L0r+uFmDUb4+7kWBJgmD5RK82rWvesK9112vJftLVLp/o6+dOXJBfYbiV5zvKNG0isjsGH2R78qn75BZDz8gG4NS2ul3zl98FjHlen6BmSTJkIPVA1sxueLqf9FCoi6UhIVECINP7cgEVxrH1hsbbtToDpnP5643fq92LHUPDvWeNZt0bbqX9KigwjMcb9w3j8saxe9PnBOkTM5n8R0TuFny89957eP3115GVlYXevXvj3XffxcCBAy/5dQw+XIQUGjq1HTj9u37Kbpv+1v1+UodAel+0C7BpcMEufiIiqzPn+m2VImNffPEFZs6ciblz52LQoEF4++23MW7cOKSlpSE2NtYa35IcjWTKy6wY2WxBpv4lj7bN9yIiohaxSr/sm2++iQcffBD/93//h+7du6sgJDAwEP/+97+t8e2IiIjInYOPyspKbN++HaNHn/8U6unpqe6npqZa+tsRERGRk7H4sMuZM2dQU1ODuLi6a4TI/QMHDLMSTFRUVKjNdMyIiIiIXJfd0+Fnz56tElS0LTEx0d5NIiIiImcKPqKjo+Hl5YXs7Ow6x+V+fHz8Bc+fNWuWyozVtoyMDEs3iYiIiFw5+PD19UW/fv2wcuVK47Ha2lp1PyUl5YLn+/n5qSk5phsRERG5LqtMtZVptpMnT0b//v1VbQ+ZaltSUqJmvxAREZF7s0rwceuttyI3NxfPPPOMKjJ2+eWXY/ny5RckoRIREZH7YXl1IiIisun12+6zXYiIiMi9MPggIiIim2LwQURERDbF4IOIiIicf7ZLS2j5ryyzTkRE5Dy063ZT5rE4XPBRVFSkbllmnYiIyPnIdVxmvTjVVFuphpqZmYmQkBB4eHhYPCqToEZKuLvzNF6+D+fxvdDj+6DH90GP78N5fC+a/j5IOCGBR0JCglrN3ql6PqTBbdq0ser3YBl3Pb4P5/G90OP7oMf3QY/vw3l8L5r2Plyqx0PDhFMiIiKyKQYfREREZFNuFXzICrrPPvusunVnfB/O43uhx/dBj++DHt+H8/heWOd9cLiEUyIiInJtbtXzQURERPbH4IOIiIhsisEHERER2RSDDyIiIrIptwk+3nvvPbRr1w7+/v4YNGgQtmzZAnfz3HPPqaqxplvXrl3h6tatW4frrrtOVd2Tn3np0qV1Hpec62eeeQatWrVCQEAARo8ejUOHDsEd34t77733gnPk6quvhiuZPXs2BgwYoKoox8bG4oYbbkBaWlqd55SXl2Pq1KmIiopCcHAwJk2ahOzsbLiaprwXV1555QXnxEMPPQRX8v7776NXr17GAlopKSlYtmyZ250P71/ifbDkueAWwccXX3yBmTNnqmlCO3bsQO/evTFu3Djk5OTA3fTo0QOnT582br/++itcXUlJifqdSwDakNdeew3vvPMO5s6di82bNyMoKEidH/IHx93eCyHBhuk5snDhQriStWvXqgvJpk2b8Msvv6Cqqgpjx45V741mxowZ+O6777B48WL1fFnyYeLEiXA1TXkvxIMPPljnnJD/M65Eqmq/8sor2L59O7Zt24aRI0diwoQJ2Lt3r1udD5d6Hyx6LujcwMCBA3VTp0413q+pqdElJCToZs+erXMnzz77rK537946dyan/JIlS4z3a2trdfHx8brXX3/deCw/P1/n5+enW7hwoc6d3gsxefJk3YQJE3TuJCcnR70Xa9euNf7+fXx8dIsXLzY+Z//+/eo5qampOnd6L8SIESN0jz32mM7dRERE6D7++GO3Ph9M3wdLnwsu3/NRWVmpojjpSjddP0bup6amwt3IcIJ0uXfo0AF33nkn0tPT4c6OHTuGrKysOueHrE0gQ3PueH6INWvWqC74Ll264OGHH0ZeXh5cWUFBgbqNjIxUt/L3QnoATM8JGZ5MSkpy+XOi/nuh+fzzzxEdHY2ePXti1qxZKC0thauqqanBokWLVO+PDDu46/lQU+99sPS54HALy1namTNn1JsYFxdX57jcP3DgANyJXFDnz5+vLirSXfb8889j2LBh2LNnjxrzdUcSeIiGzg/tMXciQy7Sndy+fXscOXIEf/vb3zB+/Hj1R9bLywuuRlbRnj59Oq644gr1x1TI793X1xfh4eFudU409F6IO+64A23btlUfWnbt2oUnnnhC5YV8/fXXcCW7d+9WF1kZbpW8jiVLlqB79+7YuXOnW50Puxt5Hyx9Lrh88EHnyUVEI0lFEozIifTll1/i/vvvt2vbyDHcdtttxv3LLrtMnScdO3ZUvSGjRo2Cq5F8Bwm+3SH3qbnvxZQpU+qcE5KYLeeCBKdybrgK+VAmgYb0/nz11VeYPHmyyu9wN10aeR8kALHkueDywy7SPSSf2OpnJsv9+Ph4uDOJ5Dt37ozDhw/DXWnnAM+PhsnwnPwfcsVz5NFHH8X333+P1atXq0Q7jfzeZbg2Pz/fbc6Jxt6LhsiHFuFq54T0bnTq1An9+vVTs4AkMfuf//yn250Pvo28D5Y+F1w++JA3Ut7ElStX1ulelPum41juqLi4WEWsEr26KxlekD8gpudHYWGhmvXi7ueHOHnypMr5cKVzRHJt5WIr3cmrVq1S54Ap+Xvh4+NT55yQrmXJj3K1c+JS70VD5FOxcKVzoiFynaioqHCr8+Fi74PFzwWdG1i0aJGavTB//nzdvn37dFOmTNGFh4frsrKydO7k8ccf161Zs0Z37Ngx3YYNG3SjR4/WRUdHqwx3V1ZUVKT77bff1Can/Jtvvqn2T5w4oR5/5ZVX1PnwzTff6Hbt2qVme7Rv315XVlamc6f3Qh7785//rDL45RxZsWKFrm/fvrrk5GRdeXm5zlU8/PDDurCwMPV/4fTp08attLTU+JyHHnpIl5SUpFu1apVu27ZtupSUFLW5mku9F4cPH9a98MIL6j2Qc0L+j3To0EE3fPhwnSt58skn1Qwf+Rnlb4Dc9/Dw0P38889udT48eZH3wdLnglsEH+Ldd99VJ4+vr6+aertp0yadu7n11lt1rVq1Uu9B69at1X05oVzd6tWr1YW2/ibTSrXptk8//bQuLi5OBamjRo3SpaWl6dztvZALztixY3UxMTFqamHbtm11Dz74oMsF6Q39/LLNmzfP+BwJPB955BE1zTAwMFB34403qouyq7nUe5Genq4uLpGRker/RqdOnXR/+ctfdAUFBTpXct9996nzXf42yvkvfwO0wMOdzof7LvI+WPpc8JB/zO8vISIiImoel8/5ICIiIsfC4IOIiIhsisEHERER2RSDDyIiIrIpBh9ERERkUww+iIiIyKYYfBAREZFNMfggIiIim2LwQURERDbF4IOIiIhsisEHERER2RSDDyIiIoIt/X+KHEWU3lQ60wAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501/501 [==============================] - 9s 17ms/step\n",
      "Fold 1 Train Predictions Sample:\n",
      "[[0.9374042  0.04588912 0.01670663]\n",
      " [0.937033   0.04633325 0.01663374]\n",
      " [0.93555886 0.04734727 0.01709395]\n",
      " [0.93611115 0.0467328  0.01715612]\n",
      " [0.9386438  0.04490959 0.01644652]\n",
      " [0.9371198  0.04599895 0.01688121]\n",
      " [0.93805623 0.04528413 0.01665966]\n",
      " [0.9376368  0.04569978 0.01666338]\n",
      " [0.9390368  0.04442951 0.01653369]\n",
      " [0.9389319  0.04463656 0.01643153]\n",
      " [0.93702036 0.04604866 0.01693102]\n",
      " [0.9368189  0.04633739 0.01684377]\n",
      " [0.935906   0.04694071 0.01715322]\n",
      " [0.9315665  0.05062364 0.01780984]\n",
      " [0.92832005 0.05329537 0.01838453]\n",
      " [0.9275635  0.05440643 0.01802999]\n",
      " [0.9242924  0.05690985 0.01879779]\n",
      " [0.9274947  0.0541892  0.01831616]\n",
      " [0.9306186  0.0515843  0.01779708]\n",
      " [0.92852706 0.05338535 0.01808756]\n",
      " [0.9279941  0.05364225 0.01836367]\n",
      " [0.92596513 0.05565716 0.01837769]\n",
      " [0.9255103  0.05606616 0.01842348]\n",
      " [0.92636466 0.0553419  0.01829353]\n",
      " [0.9268714  0.05505997 0.01806862]\n",
      " [0.9259481  0.05559597 0.01845589]\n",
      " [0.9278329  0.05415467 0.01801243]\n",
      " [0.9272572  0.05464774 0.01809517]\n",
      " [0.927638   0.05445954 0.01790247]\n",
      " [0.9255947  0.05615311 0.01825221]\n",
      " [0.92517376 0.05634942 0.01847674]\n",
      " [0.926037   0.05587524 0.01808771]\n",
      " [0.9216021  0.05916971 0.01922821]\n",
      " [0.924051   0.05750732 0.01844171]\n",
      " [0.9237547  0.05754964 0.01869569]\n",
      " [0.93109727 0.05147136 0.01743144]\n",
      " [0.93126297 0.05111822 0.01761889]\n",
      " [0.929512   0.0526015  0.01788652]\n",
      " [0.92809254 0.05387194 0.0180355 ]\n",
      " [0.92952335 0.05260116 0.01787552]\n",
      " [0.92865086 0.05366484 0.01768427]\n",
      " [0.9253821  0.05598375 0.01863416]\n",
      " [0.9296282  0.0527009  0.01767084]\n",
      " [0.92799366 0.05413027 0.0178761 ]\n",
      " [0.9276906  0.05423835 0.01807105]\n",
      " [0.92911154 0.05327092 0.01761746]\n",
      " [0.92887306 0.05349429 0.01763275]\n",
      " [0.9329859  0.04963186 0.0173822 ]\n",
      " [0.93773615 0.0458473  0.01641655]\n",
      " [0.9358139  0.04715852 0.0170275 ]]\n",
      "Fold 1 Train Predictions Variance: 1.564152e-01\n",
      "每列的均值: [0.8801503  0.09881454 0.02103653]\n",
      "每列的最大值: [0.9959132  0.38708106 0.07329735]\n",
      "每列的最小值: [0.5723373  0.0019819  0.00210479]\n",
      "每列中 1 的次数: [8013.    0.    0.]\n",
      "\n",
      "--- Fold 2 ---\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_33 (LSTM)              (None, 256)               294912    \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_60 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 3)                 771       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,475\n",
      "Trainable params: 361,475\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "1002/1002 - 53s - loss: 67.4720 - accuracy: 0.7225 - class1_accuracy: 0.2021 - class2_accuracy: 0.3310 - class0_accuracy: 0.8220 - val_loss: 1.1779 - val_accuracy: 0.9103 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9108 - lr: 0.0010 - 53s/epoch - 53ms/step\n",
      "Epoch 2/200\n",
      "1002/1002 - 52s - loss: 69.5965 - accuracy: 0.7219 - class1_accuracy: 0.4493 - class2_accuracy: 0.2300 - class0_accuracy: 0.8093 - val_loss: 1.1405 - val_accuracy: 0.8997 - val_class1_accuracy: 0.1479 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9132 - lr: 0.0010 - 52s/epoch - 52ms/step\n",
      "Epoch 3/200\n",
      "1002/1002 - 50s - loss: 80.3282 - accuracy: 0.6925 - class1_accuracy: 0.2303 - class2_accuracy: 0.2980 - class0_accuracy: 0.8056 - val_loss: 0.7643 - val_accuracy: 0.8041 - val_class1_accuracy: 0.0910 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9135 - lr: 0.0010 - 50s/epoch - 50ms/step\n",
      "Epoch 4/200\n",
      "1002/1002 - 50s - loss: 51.4249 - accuracy: 0.6580 - class1_accuracy: 0.2014 - class2_accuracy: 0.1418 - class0_accuracy: 0.8087 - val_loss: 0.9739 - val_accuracy: 0.8819 - val_class1_accuracy: 0.0397 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9093 - lr: 0.0010 - 50s/epoch - 50ms/step\n",
      "Epoch 5/200\n",
      "1002/1002 - 50s - loss: 47.7381 - accuracy: 0.5552 - class1_accuracy: 0.1282 - class2_accuracy: 0.1786 - class0_accuracy: 0.7896 - val_loss: 0.9719 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 50s/epoch - 50ms/step\n",
      "Epoch 6/200\n",
      "1002/1002 - 50s - loss: 31.4203 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.6708 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 50s/epoch - 49ms/step\n",
      "Epoch 7/200\n",
      "1002/1002 - 48s - loss: 36.9278 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.5631 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 48s/epoch - 48ms/step\n",
      "Epoch 8/200\n",
      "1002/1002 - 49s - loss: 40.4180 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.5161 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 49s/epoch - 49ms/step\n",
      "Epoch 9/200\n",
      "1002/1002 - 49s - loss: 42.4851 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4938 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 49s/epoch - 48ms/step\n",
      "Epoch 10/200\n",
      "1002/1002 - 50s - loss: 43.6595 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4825 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 50s/epoch - 50ms/step\n",
      "Epoch 11/200\n",
      "1002/1002 - 51s - loss: 44.3107 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4767 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 51s/epoch - 51ms/step\n",
      "Epoch 12/200\n",
      "1002/1002 - 51s - loss: 44.6669 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4736 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 51s/epoch - 50ms/step\n",
      "Epoch 13/200\n",
      "1002/1002 - 49s - loss: 44.8609 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4720 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 49s/epoch - 49ms/step\n",
      "Epoch 14/200\n",
      "1002/1002 - 47s - loss: 44.9666 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4711 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 47s/epoch - 47ms/step\n",
      "Epoch 15/200\n",
      "1002/1002 - 46s - loss: 45.0243 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4706 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 46s/epoch - 46ms/step\n",
      "Epoch 16/200\n",
      "1002/1002 - 53s - loss: 45.0561 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4704 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 53s/epoch - 53ms/step\n",
      "Epoch 17/200\n",
      "1002/1002 - 48s - loss: 45.0737 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4702 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 48s/epoch - 48ms/step\n",
      "Epoch 18/200\n",
      "1002/1002 - 52s - loss: 45.0836 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4701 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 52s/epoch - 52ms/step\n",
      "Epoch 19/200\n",
      "1002/1002 - 56s - loss: 45.0892 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4701 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 56s/epoch - 56ms/step\n",
      "Epoch 20/200\n",
      "1002/1002 - 57s - loss: 45.0925 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4701 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 57s/epoch - 57ms/step\n",
      "Epoch 21/200\n",
      "1002/1002 - 53s - loss: 45.0944 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4700 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 53s/epoch - 53ms/step\n",
      "Epoch 22/200\n",
      "1002/1002 - 54s - loss: 45.0955 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4700 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 0.0010 - 54s/epoch - 54ms/step\n",
      "Epoch 23/200\n",
      "1002/1002 - 53s - loss: 45.1224 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4705 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 5.0000e-04 - 53s/epoch - 53ms/step\n",
      "Epoch 24/200\n",
      "1002/1002 - 53s - loss: 45.0863 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4709 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 5.0000e-04 - 53s/epoch - 53ms/step\n",
      "Epoch 25/200\n",
      "1002/1002 - 54s - loss: 45.0599 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4711 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 5.0000e-04 - 54s/epoch - 54ms/step\n",
      "Epoch 26/200\n",
      "1002/1002 - 54s - loss: 45.0726 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4714 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 2.5000e-04 - 54s/epoch - 54ms/step\n",
      "Epoch 27/200\n",
      "1002/1002 - 50s - loss: 45.0516 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4716 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 2.5000e-04 - 50s/epoch - 50ms/step\n",
      "Epoch 28/200\n",
      "1002/1002 - 53s - loss: 45.0338 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4718 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 2.5000e-04 - 53s/epoch - 53ms/step\n",
      "Epoch 29/200\n",
      "1002/1002 - 55s - loss: 45.0402 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4720 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.2500e-04 - 55s/epoch - 55ms/step\n",
      "Epoch 30/200\n",
      "1002/1002 - 50s - loss: 45.0294 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4721 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.2500e-04 - 50s/epoch - 50ms/step\n",
      "Epoch 31/200\n",
      "1002/1002 - 49s - loss: 45.0194 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4722 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.2500e-04 - 49s/epoch - 48ms/step\n",
      "Epoch 32/200\n",
      "1002/1002 - 55s - loss: 45.0226 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4723 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 6.2500e-05 - 55s/epoch - 55ms/step\n",
      "Epoch 33/200\n",
      "1002/1002 - 51s - loss: 45.0172 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4723 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 6.2500e-05 - 51s/epoch - 51ms/step\n",
      "Epoch 34/200\n",
      "1002/1002 - 53s - loss: 45.0121 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4724 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 6.2500e-05 - 53s/epoch - 53ms/step\n",
      "Epoch 35/200\n",
      "1002/1002 - 50s - loss: 45.0136 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4724 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.1250e-05 - 50s/epoch - 50ms/step\n",
      "Epoch 36/200\n",
      "1002/1002 - 52s - loss: 45.0109 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.1250e-05 - 52s/epoch - 51ms/step\n",
      "Epoch 37/200\n",
      "1002/1002 - 54s - loss: 45.0084 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.1250e-05 - 54s/epoch - 53ms/step\n",
      "Epoch 38/200\n",
      "1002/1002 - 50s - loss: 45.0093 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.5625e-05 - 50s/epoch - 50ms/step\n",
      "Epoch 39/200\n",
      "1002/1002 - 51s - loss: 45.0080 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.5625e-05 - 51s/epoch - 51ms/step\n",
      "Epoch 40/200\n",
      "1002/1002 - 49s - loss: 45.0067 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 1.5625e-05 - 49s/epoch - 49ms/step\n",
      "Epoch 41/200\n",
      "1002/1002 - 50s - loss: 45.0072 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 7.8125e-06 - 50s/epoch - 50ms/step\n",
      "Epoch 42/200\n",
      "1002/1002 - 49s - loss: 45.0065 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4725 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 7.8125e-06 - 49s/epoch - 49ms/step\n",
      "Epoch 43/200\n",
      "1002/1002 - 49s - loss: 45.0058 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4726 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 7.8125e-06 - 49s/epoch - 48ms/step\n",
      "Epoch 44/200\n",
      "1002/1002 - 50s - loss: 45.0060 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4726 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.9063e-06 - 50s/epoch - 50ms/step\n",
      "Epoch 45/200\n",
      "1002/1002 - 50s - loss: 45.0056 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4726 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.9063e-06 - 50s/epoch - 50ms/step\n",
      "Epoch 46/200\n",
      "1002/1002 - 49s - loss: 45.0053 - accuracy: 0.7705 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.7705 - val_loss: 0.4726 - val_accuracy: 0.9109 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.9109 - lr: 3.9063e-06 - 49s/epoch - 49ms/step\n",
      "Epoch 47/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[243], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m27\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 在验证集上评估模型\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#val_loss = model.evaluate(val_dataset, verbose=0)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#print(f\"Fold {fold} Validation Loss: {val_loss:.6f}\")\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# 可视化损失\u001b[39;00m\n\u001b[0;32m     36\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 将总样本索引构造为一个数组\n",
    "indices = np.arange(total_samples)\n",
    "for train_index, val_index in tscv.split(indices):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "\n",
    "\n",
    "\n",
    "    # 构造训练集和验证集生成器（利用生成器按需生成数据）\n",
    "    train_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: sequence_generator(train_index, X_all, y_all, seq_length),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: sequence_generator(val_index, X_all, y_all, seq_length),\n",
    "        output_signature=output_signature\n",
    "    ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # 构建模型，input_shape 为 (seq_length, input_dim)\n",
    "    model = build_model(input_shape=(seq_length, input_dim))\n",
    "    model.summary()\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=27, restore_best_weights=True)\n",
    "    # 训练模型\n",
    "    history = model.fit(train_dataset,\n",
    "                        epochs=num_epochs,\n",
    "                        validation_data=val_dataset,callbacks=[early_stopping,reduce_lr],class_weight=class_weight,\n",
    "                        verbose=2)\n",
    "    # 在验证集上评估模型\n",
    "    #val_loss = model.evaluate(val_dataset, verbose=0)\n",
    "    #print(f\"Fold {fold} Validation Loss: {val_loss:.6f}\")\n",
    "    \n",
    "        # 可视化损失\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # --------------------------------------\n",
    "    # 使用训练集进行预测，观察输出是否一致\n",
    "    # --------------------------------------\n",
    "    #train_predictions = model.predict(train_dataset, batch_size=batch_size)\n",
    "    train_predictions = model.predict(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    print(f\"Fold {fold} Train Predictions Sample:\")\n",
    "    print(train_predictions[:50])\n",
    "    print(f\"Fold {fold} Train Predictions Variance: {np.var(train_predictions):.6e}\")\n",
    "    \n",
    "    predict_statistics(train_predictions)\n",
    "\n",
    "\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12204/12204 [==============================] - 397s 33ms/step\n",
      "Fold 1 Train Predictions Sample:\n",
      "[[0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]\n",
      " [0.8642346  0.06956238 0.06620304]]\n",
      "Fold 1 Train Predictions Variance: 1.409300e-01\n"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    # --------------------------------------\n",
    "    # 使用训练集进行预测，观察输出是否一致\n",
    "    # --------------------------------------\n",
    "train_predictions = model.predict(val_dataset, batch_size=batch_size)\n",
    "print(f\"Fold {fold} Train Predictions Sample:\")\n",
    "print(train_predictions[:50])\n",
    "print(f\"Fold {fold} Train Predictions Variance: {np.var(train_predictions):.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每列的均值: [0.864012   0.06940867 0.06633272]\n",
      "每列的最大值: [0.8642346  0.06956238 0.06620304]\n",
      "每列的最小值: [0.8642346  0.06956238 0.06620304]\n",
      "每列中 1 的次数: [195250.      0.      0.]\n"
     ]
    }
   ],
   "source": [
    "predict_statistics(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_statistics(predictions):\n",
    "    # 计算每列的统计信息\n",
    "    means = np.mean(predictions, axis=0)  # 均值\n",
    "    maxs = np.max(predictions, axis=0)    # 最大值\n",
    "    mins = np.min(predictions, axis=0)    # 最小值\n",
    "    # 打印结果\n",
    "    print(\"每列的均值:\", means)\n",
    "    print(\"每列的最大值:\", maxs)\n",
    "    print(\"每列的最小值:\", mins)\n",
    "    #找到每行最大值的索引\n",
    "    max_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # 创建一个全零数组，shape 与 train_predictions 相同\n",
    "    one_hot = np.zeros_like(predictions)\n",
    "\n",
    "    # 将最大值位置标记为 1\n",
    "    one_hot[np.arange(len(max_indices)), max_indices] = 1\n",
    "\n",
    "    # 统计每列中 1 的次数\n",
    "    counts = np.sum(one_hot, axis=0)\n",
    "\n",
    "    # 打印结果\n",
    "    print(\"每列中 1 的次数:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train data(8175, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每列的均值: [0.73707956 0.14204124 0.12087925]\n",
    "\n",
    "每列的最大值: [1.         1.         0.99999857]\n",
    "\n",
    "每列的最小值: [5.3541595e-11 1.6682273e-14 3.4969445e-20]\n",
    "\n",
    "每列中 1 的次数: [6052. 1163.  960.]\n",
    "\n",
    "--- Fold 1 ---\n",
    "Model: \"sequential_27\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " lstm_29 (LSTM)              (None, 256)               294912    \n",
    "                                                                 \n",
    " dropout_53 (Dropout)        (None, 256)               0         \n",
    "                                                                 \n",
    " dense_53 (Dense)            (None, 256)               65792     \n",
    "                                                                 \n",
    " dropout_54 (Dropout)        (None, 256)               0         \n",
    "                                                                 \n",
    " dense_54 (Dense)            (None, 3)                 771       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 361,475\n",
    "Trainable params: 361,475\n",
    "Non-trainable params: 0\n",
    "_________________________________________________________________\n",
    "Epoch 1/200\n",
    "c:\\Users\\zhaoz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:2319: UserWarning: Metric PerClassPrecision implements a `reset_states()` method; rename it to `reset_state()` (without the final \"s\"). The name `reset_states()` has been deprecated to improve API consistency.\n",
    "  m.reset_state()\n",
    "12204/12204 - 683s - loss: 152.9196 - accuracy: 0.8529 - class1_accuracy: 0.2046 - class2_accuracy: 0.2930 - class0_accuracy: 0.8737 - val_loss: 1.1338 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 0.0010 - 683s/epoch - 56ms/step\n",
    "Epoch 2/200\n",
    "12204/12204 - 715s - loss: 170.4310 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.1395 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 0.0010 - 715s/epoch - 59ms/step\n",
    "Epoch 3/200\n",
    "12204/12204 - 719s - loss: 170.6495 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.1395 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 0.0010 - 719s/epoch - 59ms/step\n",
    "Epoch 4/200\n",
    "12204/12204 - 719s - loss: 170.6497 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.1395 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 0.0010 - 719s/epoch - 59ms/step\n",
    "Epoch 5/200\n",
    "12204/12204 - 702s - loss: 177.3042 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0857 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 702s/epoch - 57ms/step\n",
    "Epoch 6/200\n",
    "12204/12204 - 701s - loss: 173.1981 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0839 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 701s/epoch - 57ms/step\n",
    "Epoch 7/200\n",
    "12204/12204 - 698s - loss: 173.0595 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0838 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 698s/epoch - 57ms/step\n",
    "Epoch 8/200\n",
    "12204/12204 - 697s - loss: 173.0553 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0838 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 697s/epoch - 57ms/step\n",
    "Epoch 9/200\n",
    "12204/12204 - 696s - loss: 173.0552 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0837 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 696s/epoch - 57ms/step\n",
    "Epoch 10/200\n",
    "12204/12204 - 696s - loss: 173.0552 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0837 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 696s/epoch - 57ms/step\n",
    "Epoch 11/200\n",
    "12204/12204 - 695s - loss: 173.0553 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0837 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 5.0000e-04 - 695s/epoch - 57ms/step\n",
    "Epoch 12/200\n",
    "12204/12204 - 695s - loss: 178.8430 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0547 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 695s/epoch - 57ms/step\n",
    "Epoch 13/200\n",
    "12204/12204 - 694s - loss: 175.0541 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0493 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 694s/epoch - 57ms/step\n",
    "Epoch 14/200\n",
    "12204/12204 - 691s - loss: 174.3617 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0482 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 691s/epoch - 57ms/step\n",
    "Epoch 15/200\n",
    "12204/12204 - 692s - loss: 174.2376 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0480 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 692s/epoch - 57ms/step\n",
    "Epoch 16/200\n",
    "12204/12204 - 693s - loss: 174.2161 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0480 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 693s/epoch - 57ms/step\n",
    "Epoch 17/200\n",
    "12204/12204 - 696s - loss: 174.2126 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0480 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 696s/epoch - 57ms/step\n",
    "Epoch 18/200\n",
    "12204/12204 - 695s - loss: 174.2122 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0480 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 2.5000e-04 - 695s/epoch - 57ms/step\n",
    "Epoch 19/200\n",
    "12204/12204 - 698s - loss: 177.7649 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0383 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 698s/epoch - 57ms/step\n",
    "Epoch 20/200\n",
    "12204/12204 - 695s - loss: 175.9796 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0339 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 695s/epoch - 57ms/step\n",
    "Epoch 21/200\n",
    "12204/12204 - 694s - loss: 175.2157 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0324 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 694s/epoch - 57ms/step\n",
    "Epoch 22/200\n",
    "12204/12204 - 738s - loss: 174.8867 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0315 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 738s/epoch - 60ms/step\n",
    "Epoch 23/200\n",
    "12204/12204 - 764s - loss: 174.7498 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0311 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 764s/epoch - 63ms/step\n",
    "Epoch 24/200\n",
    "12204/12204 - 767s - loss: 174.6921 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0310 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 767s/epoch - 63ms/step\n",
    "Epoch 25/200\n",
    "12204/12204 - 764s - loss: 174.6681 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0310 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 764s/epoch - 63ms/step\n",
    "Epoch 26/200\n",
    "12204/12204 - 773s - loss: 174.6584 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0309 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 773s/epoch - 63ms/step\n",
    "Epoch 27/200\n",
    "12204/12204 - 810s - loss: 174.6547 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0309 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.2500e-04 - 810s/epoch - 66ms/step\n",
    "Epoch 28/200\n",
    "12204/12204 - 744s - loss: 176.5448 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0282 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 744s/epoch - 61ms/step\n",
    "Epoch 29/200\n",
    "12204/12204 - 711s - loss: 175.9405 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0263 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 711s/epoch - 58ms/step\n",
    "Epoch 30/200\n",
    "12204/12204 - 638s - loss: 175.5460 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0253 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 638s/epoch - 52ms/step\n",
    "Epoch 31/200\n",
    "12204/12204 - 615s - loss: 175.2888 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0244 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 615s/epoch - 50ms/step\n",
    "Epoch 32/200\n",
    "12204/12204 - 612s - loss: 175.1230 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0241 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 612s/epoch - 50ms/step\n",
    "Epoch 33/200\n",
    "12204/12204 - 615s - loss: 175.0097 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0237 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 615s/epoch - 50ms/step\n",
    "Epoch 34/200\n",
    "12204/12204 - 615s - loss: 174.9389 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0234 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 615s/epoch - 50ms/step\n",
    "Epoch 35/200\n",
    "12204/12204 - 660s - loss: 174.8929 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0232 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 660s/epoch - 54ms/step\n",
    "Epoch 36/200\n",
    "12204/12204 - 644s - loss: 174.8630 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0232 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 644s/epoch - 53ms/step\n",
    "Epoch 37/200\n",
    "12204/12204 - 626s - loss: 174.8437 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0231 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 626s/epoch - 51ms/step\n",
    "Epoch 38/200\n",
    "12204/12204 - 623s - loss: 174.8313 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0231 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 623s/epoch - 51ms/step\n",
    "Epoch 39/200\n",
    "12204/12204 - 629s - loss: 174.8232 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0231 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 629s/epoch - 52ms/step\n",
    "Epoch 40/200\n",
    "12204/12204 - 626s - loss: 174.8181 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0231 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 6.2500e-05 - 626s/epoch - 51ms/step\n",
    "Epoch 41/200\n",
    "12204/12204 - 627s - loss: 175.7815 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0225 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 627s/epoch - 51ms/step\n",
    "Epoch 42/200\n",
    "12204/12204 - 623s - loss: 175.6055 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0217 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 623s/epoch - 51ms/step\n",
    "Epoch 43/200\n",
    "12204/12204 - 628s - loss: 175.4650 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0215 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 628s/epoch - 51ms/step\n",
    "Epoch 44/200\n",
    "12204/12204 - 627s - loss: 175.3501 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0211 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 627s/epoch - 51ms/step\n",
    "Epoch 45/200\n",
    "12204/12204 - 629s - loss: 175.2576 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0207 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 629s/epoch - 52ms/step\n",
    "Epoch 46/200\n",
    "12204/12204 - 628s - loss: 175.1831 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0203 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 628s/epoch - 51ms/step\n",
    "Epoch 47/200\n",
    "12204/12204 - 632s - loss: 175.1228 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0202 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 632s/epoch - 52ms/step\n",
    "Epoch 48/200\n",
    "12204/12204 - 631s - loss: 175.0740 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0200 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 631s/epoch - 52ms/step\n",
    "Epoch 49/200\n",
    "12204/12204 - 635s - loss: 175.0348 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0198 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 635s/epoch - 52ms/step\n",
    "Epoch 50/200\n",
    "12204/12204 - 663s - loss: 174.9981 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0200 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 663s/epoch - 54ms/step\n",
    "Epoch 51/200\n",
    "12204/12204 - 650s - loss: 174.9728 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0198 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 650s/epoch - 53ms/step\n",
    "Epoch 52/200\n",
    "12204/12204 - 706s - loss: 174.9522 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0198 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 3.1250e-05 - 706s/epoch - 58ms/step\n",
    "Epoch 53/200\n",
    "12204/12204 - 693s - loss: 175.4286 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0195 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.5625e-05 - 693s/epoch - 57ms/step\n",
    "Epoch 54/200\n",
    "12204/12204 - 640s - loss: 175.3744 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0193 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.5625e-05 - 640s/epoch - 52ms/step\n",
    "Epoch 55/200\n",
    "12204/12204 - 632s - loss: 175.3259 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0191 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.5625e-05 - 632s/epoch - 52ms/step\n",
    "Epoch 56/200\n",
    "12204/12204 - 778s - loss: 175.2822 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0189 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.5625e-05 - 778s/epoch - 64ms/step\n",
    "Epoch 57/200\n",
    "12204/12204 - 821s - loss: 175.2428 - accuracy: 0.8690 - class1_accuracy: 0.0000e+00 - class2_accuracy: 0.0000e+00 - class0_accuracy: 0.8690 - val_loss: 1.0187 - val_accuracy: 0.6556 - val_class1_accuracy: 0.0000e+00 - val_class2_accuracy: 0.0000e+00 - val_class0_accuracy: 0.6556 - lr: 1.5625e-05 - 821s/epoch - 67ms/step\n",
    "Epoch 58/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 49037, 49038, 49039])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49520,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造训练集和验证集生成器（利用生成器按需生成数据）\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: sequence_generator(indices, X_all, y_all, seq_length),\n",
    "        output_signature=output_signature\n",
    ").batch(batch_size).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3065/3065 [==============================] - 40s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每列的均值: [0.29894814 0.41583756 0.2852083 ]\n",
      "每列的最大值: [1. 1. 1.]\n",
      "每列的最小值: [5.3541595e-11 4.5379284e-19 8.7022583e-21]\n",
      "每列中 1 的次数: [13235. 21828. 13977.]\n"
     ]
    }
   ],
   "source": [
    "test_predictions.shape\n",
    "predict_statistics(test_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
